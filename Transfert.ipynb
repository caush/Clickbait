{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"Clickbait\"\n",
    "checkpoint = \"cmarkea/distilcamembert-base\"   # cmarkea/distilcamembert-base-sentiment \"cmarkea/distilcamembert-base\" \"camembert-base\" \"microsoft/Multilingual-MiniLM-L12-H384\"\n",
    "problem_type =  \"single_label_classification\" # Valeurs possibles : \"regression\", \"single_label_classification\", \"multi_label_classification\"\n",
    "num_labels = 3                                # 5\n",
    "dataCuratedPath = \"Data/Curated\"              # Données pour tous les sites du 1er janvier 2021 au 30 avril 2021\n",
    "                                              # Données juste pour le site \"Actualités, trucs et astuces\" entre le 1er mai et le 31 juillet 2021.\n",
    "                                              # Les colonnes sont les suivantes, séparées par des \";\" :\n",
    "                                              #    Page name;Title;Publish time;People Reached;Link Clicks\n",
    "LCfraction = 'truthMean'                      # 'LCFraction'\n",
    "removeColumns = ['postText', 'targetTitle', 'postTimestamp', 'targetKeywords', 'targetDescription', 'id', 'truthMean', 'attention_mask'] # Il faut enlever les colonnes de type texte\n",
    "splitFactor = 0.1                             # Proportion de données réservées pour les tests\n",
    "push_to_hub = False\n",
    "learning_rate = 5e-5\n",
    "weight_decay=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "from transformers import CamembertTokenizerFast, CamembertTokenizer\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cmarkea/distilcamembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at cmarkea/distilcamembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# checkpoint = \"cmarkea/distilcamembert-base\"\n",
    "# checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   checkpoint,\n",
    "   problem_type=problem_type, # on pourra enlever ce paramètre si ca marche plus\n",
    "   num_labels = num_labels)\n",
    "# model = BertForSequenceClassification.from_pretrained(checkpoint,\n",
    "# problem_type=\"multi_label_classification\", # ne sert à rien ?\n",
    "# num_labels = 5                             # 5 est la seule valeur possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated = ds.load_from_disk(dataCuratedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "longueur= len(curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie=sorted(curated[LCfraction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21346ee49d0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1872x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(26, 20))\n",
    "plt.title(\"Plotting 1-D array\")\n",
    "plt.xlabel(\"Ordered Title\")\n",
    "plt.ylabel(LCfraction)\n",
    "x = np.array(range(0, longueur))\n",
    "y = np.array(trie)\n",
    "plt.plot(x, y, color = \"red\", marker = \"o\", label = \"Array elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.199999999979999, 0.399999999979999]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncf=[None] * (num_labels-1)\n",
    "for i in range(num_labels-1):\n",
    "    ncf[i] = trie[longueur * (i+1) // num_labels]\n",
    "ncf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    tokenized_sample = tokenizer(\n",
    "            sample['postText'],  #+ \". Paru dans \" + sample[\"Page name\"], #+ \", le \" + sample[\"Publish time\"]\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=30)\n",
    "    fraction = sample[LCfraction]\n",
    "    if num_labels == 1:\n",
    "        tokenized_sample[\"labels\"] = fraction\n",
    "    else:\n",
    "        tokenized_sample[\"labels\"] = next((x for x, val in enumerate(ncf) if fraction < val), num_labels-1)\n",
    "    return tokenized_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at Data/Curated\\cache-dcfb92c3de1ccb1c.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_curated = curated.map(tokenize) #, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['postText', 'id', 'targetTitle', 'postTimestamp', 'targetKeywords', 'targetDescription', 'truthMean', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 19538\n",
       "})"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at Data/Curated\\cache-00bc6830c2050202.arrow and Data/Curated\\cache-ec635c58374c615a.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_curated2 = tokenized_curated.train_test_split(splitFactor) # 0.2\n",
    "tokenized_curated3 = tokenized_curated2.remove_columns(removeColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_curated4 = tokenized_curated3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_curated4.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    if num_labels == 1:\n",
    "        metric = load_metric(\"mse\")\n",
    "        return metric.compute(predictions=logits, references=labels)\n",
    "    elif num_labels == 2:\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "#    recall = recall_score(y_true=labels, y_pred=predictions)\n",
    "#    precision = precision_score(y_true=labels, y_pred=predictions)\n",
    "        f1 = f1_score(y_true=labels, y_pred=predictions)\n",
    "    # return metric.compute(predictions=predictions, references=labels) # , average = 'weighted'\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1 } #\" \"matthews_correlation\":matthews_correlation precision\": precision, \"recall\": recall, \"f1\": f1, \n",
    "    else:\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        metric = load_metric(\"matthews_correlation\") # matthews_correlation, accuracy \"glue\", \"mrpc\") # , \"sst2\") #  stsb de la ouatte de phoque\n",
    "        matthews_correlation = metric.compute(predictions=predictions, references=labels)\n",
    "        # accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "        # f1 = f1_score(y_true=labels, y_pred=predictions)\n",
    "        return matthews_correlation\n",
    "        # return {\"accuracy\": accuracy}.update(matthews_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(modelPath,\n",
    "                                  evaluation_strategy= \"steps\", # \"epoch\",\n",
    "                                  eval_steps = 50, # Evaluation and Save happens every 50 steps\n",
    "                                  save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "                                  num_train_epochs = 8,\n",
    "                                  optim= 'adamw_torch',\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  weight_decay=weight_decay,\n",
    "                                  push_to_hub=push_to_hub,\n",
    "                                  metric_for_best_model = 'accuracy' if num_labels == 2 else 'mse' if num_labels == 1 else 'matthews_correlation',\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_curated4[\"train\"],\n",
    "    eval_dataset=tokenized_curated4[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)] #3\n",
    "    # optimizers=(torch.optim.AdamW, torch.optim.lr_scheduler.LambdaLR) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 17584\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17584\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\tokenization_utils_base.py:708\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=706'>707</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=707'>708</a>\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=709'>710</a>\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=710'>711</a>\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=711'>712</a>\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=712'>713</a>\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=713'>714</a>\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=714'>715</a>\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 25 at dim 2 (got 30)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\herve\\Documents\\Clickbait\\Transfert.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/herve/Documents/Clickbait/Transfert.ipynb#ch0000029?line=0'>1</a>\u001b[0m \u001b[39m# faire la boucle avec le trainer pytorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/herve/Documents/Clickbait/Transfert.ipynb#ch0000029?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\trainer.py:1374\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/trainer.py?line=1370'>1371</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/trainer.py?line=1372'>1373</a>\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/trainer.py?line=1373'>1374</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/trainer.py?line=1374'>1375</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/trainer.py?line=1375'>1376</a>\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/trainer.py?line=1376'>1377</a>\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/trainer.py?line=1377'>1378</a>\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\data\\data_collator.py:247\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=245'>246</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m--> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=246'>247</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=247'>248</a>\u001b[0m         features,\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=248'>249</a>\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=249'>250</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=250'>251</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=251'>252</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=252'>253</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=253'>254</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/data/data_collator.py?line=254'>255</a>\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2862\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=2858'>2859</a>\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[0;32m   <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=2859'>2860</a>\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m-> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=2861'>2862</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\tokenization_utils_base.py:213\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=208'>209</a>\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=210'>211</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[1;32m--> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=212'>213</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\tokenization_utils_base.py:724\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=718'>719</a>\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=719'>720</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=720'>721</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=721'>722</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=722'>723</a>\u001b[0m             )\n\u001b[1;32m--> <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=723'>724</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=724'>725</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=725'>726</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=726'>727</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/herve/anaconda3/envs/PytorchGPU/lib/site-packages/transformers/tokenization_utils_base.py?line=728'>729</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "# faire la boucle avec le trainer pytorch\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "518d7075d1893c878292e2224ccc1059d061275fbe8ab229c7a260871a84e0e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('PytorchGPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

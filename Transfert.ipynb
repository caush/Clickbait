{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"Clickbait\"\n",
    "checkpoint = \"microsoft/Multilingual-MiniLM-L12-H384\"\n",
    "                                              # \"cmarkea/distilcamembert-base-sentiment\"\n",
    "                                              # \"cmarkea/distilcamembert-base\" (accuracy=0,69)\n",
    "                                              # \"camembert-base\"\n",
    "                                              # \"microsoft/Multilingual-MiniLM-L12-H384\" (accuracy=0,64)\n",
    "                                              # \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "                                              # \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "                                              # \"bert-base-uncased\"\n",
    "problem_type =  \"single_label_classification\" # Valeurs possibles : \"regression\", \"single_label_classification\", \"multi_label_classification\"\n",
    "num_labels = 2                                # 5\n",
    "dataCuratedPath = \"Data/Curated\"              # Données pour tous les sites du 1er janvier 2021 au 30 avril 2021\n",
    "                                              # Données juste pour le site \"Actualités, trucs et astuces\" entre le 1er mai et le 31 juillet 2021.\n",
    "                                              # Les colonnes sont les suivantes, séparées par des \";\" :\n",
    "                                              #    Page name;Title;Publish time;People Reached;Link Clicks\n",
    "LCfraction = 'truthMean'\n",
    "inputColumn = 'postText'\n",
    "removeColumns = ['postText', 'targetTitle', 'targetDescription', 'id', 'truthMean'] # Il faut enlever les colonnes de type texte\n",
    "splitFactor = 0.2                             # Proportion de données réservées pour les tests\n",
    "push_to_hub = False\n",
    "learning_rate = 5e-5\n",
    "weight_decay=0.01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "modelPath = \"ClickbaitTest\"\n",
    "checkpoint = \"cmarkea/distilcamembert-base\"\n",
    "                                              # \"cmarkea/distilcamembert-base-sentiment\"\n",
    "                                              # \"cmarkea/distilcamembert-base\" (accuracy=0,69)\n",
    "                                              # \"camembert-base\"\n",
    "                                              # \"microsoft/Multilingual-MiniLM-L12-H384\" (accuracy=0,64)\n",
    "                                              # \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "                                              # \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "                                              # \"bert-base-uncased\"\n",
    "problem_type =  \"single_label_classification\" # Valeurs possibles : \"regression\", \"single_label_classification\", \"multi_label_classification\"\n",
    "num_labels = 2                                # 5\n",
    "dataCuratedPath = \"../Perroquet/Data/CuratedMean\"              # Données pour tous les sites du 1er janvier 2021 au 30 avril 2021\n",
    "                                              # Données juste pour le site \"Actualités, trucs et astuces\" entre le 1er mai et le 31 juillet 2021.\n",
    "                                              # Les colonnes sont les suivantes, séparées par des \";\" :\n",
    "                                              #    Page name;Title;Publish time;People Reached;Link Clicks\n",
    "LCfraction = 'Fraction'                       # 'LCFraction'\n",
    "inputColumn = 'Title'\n",
    "removeColumns = ['Title', 'Publish time', 'Page name', 'People Reached', 'Link Clicks', 'LCFraction', 'PRFraction', 'Fraction'] # Il faut enlever les colonnes de type texte\n",
    "splitFactor = 0.2                             # Proportion de données réservées pour les tests\n",
    "push_to_hub = False\n",
    "learning_rate = 5e-5\n",
    "weight_decay=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/Multilingual-MiniLM-L12-H384 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   checkpoint,\n",
    "   problem_type=problem_type, # on pourra enlever ce paramètre si ca marche plus\n",
    "   num_labels = num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(250037, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 84602, 4, 6868, 3244, 307, 705, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['▁Bonjour', ',', '▁comment', '▁ça', '▁va', '▁?']\n",
      "[0, 84602, 4, 6868, 3244, 307, 705, 2]\n",
      "[84602, 4, 6868, 3244, 307, 705]\n",
      "['<s>', '▁Bonjour', ',', '▁comment', '▁ça', '▁va', '▁?', '</s>']\n",
      "<s> Bonjour, comment ça va?</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Bonjour, comment ça va ?\"))\n",
    "print(tokenizer.tokenize(\"Bonjour, comment ça va ?\"))\n",
    "print(tokenizer.encode(\"Bonjour, comment ça va ?\"))\n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"Bonjour, comment ça va ?\")))\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(\"Bonjour, comment ça va ?\")))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Bonjour, comment ça va ?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated = ds.load_from_disk(dataCuratedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['postText', 'id', 'targetTitle', 'targetDescription', 'truthMean'],\n",
       "    num_rows: 19538\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postText</th>\n",
       "      <th>id</th>\n",
       "      <th>targetTitle</th>\n",
       "      <th>targetDescription</th>\n",
       "      <th>truthMean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK’s response to modern slavery leaving victim...</td>\n",
       "      <td>858462320779026432</td>\n",
       "      <td>‘Inexcusable’ failures in UK’s response to mod...</td>\n",
       "      <td>“Inexcusable” failures in the UK’s system for ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is good</td>\n",
       "      <td>858421020331560960</td>\n",
       "      <td>Donald Trump Appoints Pro-Life Advocate as Ass...</td>\n",
       "      <td>President Donald Trump has appointed pro-life ...</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The \"forgotten\" Trump roast: Relive his brutal...</td>\n",
       "      <td>858368123753435136</td>\n",
       "      <td>The ‘forgotten’ Trump roast: Relive his brutal...</td>\n",
       "      <td>President Trump won't be at this year's White ...</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meet the happiest #dog in the world!</td>\n",
       "      <td>858323428260139008</td>\n",
       "      <td>Meet The Happiest Dog In The World, Maru The H...</td>\n",
       "      <td>The article is about Maru, a husky dog who has...</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tokyo's subway is shut down amid fears over an...</td>\n",
       "      <td>858283602626347008</td>\n",
       "      <td>Tokyo's subway is shut down amid fears over an...</td>\n",
       "      <td>The temporary suspension, which lasted ten min...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19533</th>\n",
       "      <td>Brazil soccer team and pilot's final interview...</td>\n",
       "      <td>804250183642976256</td>\n",
       "      <td>NBC News Video See Brazil Soccer Team, Pilot’s...</td>\n",
       "      <td>NBC News</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19534</th>\n",
       "      <td>😱😱😱😱😱😱😱😱😱😱😱😱😱😱</td>\n",
       "      <td>804156272086020096</td>\n",
       "      <td>Politico Scoop: Eric Trump Killed Two Deer</td>\n",
       "      <td>Politico Scoop: Eric Trump Killed Two Deer</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19535</th>\n",
       "      <td>Frenchs Forest high school may have to make wa...</td>\n",
       "      <td>804149798651588608</td>\n",
       "      <td>Frenchs Forest high school may relocate to mak...</td>\n",
       "      <td>The Forest High School on Sydney's northern be...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19536</th>\n",
       "      <td>Oh Jeff… #bruh</td>\n",
       "      <td>804134698729385984</td>\n",
       "      <td>Los Angeles Rams Jeff Fisher May Think Danny W...</td>\n",
       "      <td>Los Angeles Rams news, rumors, scores, schedul...</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19537</th>\n",
       "      <td>Richard Sherman weighs in on Cam Newton’s stru...</td>\n",
       "      <td>804126501117435904</td>\n",
       "      <td>Seattle Seahawks Richard Sherman Says 'Karma' ...</td>\n",
       "      <td>Seattle Seahawks news, rumors, scores, schedul...</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19538 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                postText                  id  \\\n",
       "0      UK’s response to modern slavery leaving victim...  858462320779026432   \n",
       "1                                           this is good  858421020331560960   \n",
       "2      The \"forgotten\" Trump roast: Relive his brutal...  858368123753435136   \n",
       "3                   Meet the happiest #dog in the world!  858323428260139008   \n",
       "4      Tokyo's subway is shut down amid fears over an...  858283602626347008   \n",
       "...                                                  ...                 ...   \n",
       "19533  Brazil soccer team and pilot's final interview...  804250183642976256   \n",
       "19534                                     😱😱😱😱😱😱😱😱😱😱😱😱😱😱  804156272086020096   \n",
       "19535  Frenchs Forest high school may have to make wa...  804149798651588608   \n",
       "19536                                     Oh Jeff… #bruh  804134698729385984   \n",
       "19537  Richard Sherman weighs in on Cam Newton’s stru...  804126501117435904   \n",
       "\n",
       "                                             targetTitle  \\\n",
       "0      ‘Inexcusable’ failures in UK’s response to mod...   \n",
       "1      Donald Trump Appoints Pro-Life Advocate as Ass...   \n",
       "2      The ‘forgotten’ Trump roast: Relive his brutal...   \n",
       "3      Meet The Happiest Dog In The World, Maru The H...   \n",
       "4      Tokyo's subway is shut down amid fears over an...   \n",
       "...                                                  ...   \n",
       "19533  NBC News Video See Brazil Soccer Team, Pilot’s...   \n",
       "19534         Politico Scoop: Eric Trump Killed Two Deer   \n",
       "19535  Frenchs Forest high school may relocate to mak...   \n",
       "19536  Los Angeles Rams Jeff Fisher May Think Danny W...   \n",
       "19537  Seattle Seahawks Richard Sherman Says 'Karma' ...   \n",
       "\n",
       "                                       targetDescription  truthMean  \n",
       "0      “Inexcusable” failures in the UK’s system for ...   1.000000  \n",
       "1      President Donald Trump has appointed pro-life ...   0.133333  \n",
       "2      President Trump won't be at this year's White ...   0.400000  \n",
       "3      The article is about Maru, a husky dog who has...   0.266667  \n",
       "4      The temporary suspension, which lasted ten min...   0.000000  \n",
       "...                                                  ...        ...  \n",
       "19533                                           NBC News   0.133333  \n",
       "19534         Politico Scoop: Eric Trump Killed Two Deer   0.066667  \n",
       "19535  The Forest High School on Sydney's northern be...   0.333333  \n",
       "19536  Los Angeles Rams news, rumors, scores, schedul...   0.733333  \n",
       "19537  Seattle Seahawks news, rumors, scores, schedul...   0.066667  \n",
       "\n",
       "[19538 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelPath == \"ClickbaitTest\":\n",
    "    # La il va falloir filtrer pour Actualités, trucs et astuces\n",
    "    import re\n",
    "    pageName = 'Actualités, trucs et astuces'\n",
    "    p = re.compile(pageName)\n",
    "    curated=curated.filter(lambda x : p.match(x[\"Page name\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curated=curated.filter(lambda x : x[\"id\"]>858283602626347008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longueur= len(curated)\n",
    "longueur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie=sorted(curated[LCfraction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2559002c190>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABd4AAAR8CAYAAACDq4YDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABL/ElEQVR4nOzdf7jmd13f+dd7ZhIQECfKFE1CCGwRG/zB6pRar62WTVRC1bR19xIM0rB0I6u0uuvupS7aerXNrq5rLVUUsywCBkGr/AgtLHWyVC4VVwYX0YRFQzQhicggM/wKmmTms3+cM86d4cydc+Z8vuf7/d7343Fd55o5933PfT6H62aSPOc137taawEAAAAAAPrYN/YBAAAAAABglQjvAAAAAADQkfAOAAAAAAAdCe8AAAAAANCR8A4AAAAAAB0J7wAAAAAA0JHwDgAAu1BV/6mq/nHH53t5Vf1wr+cDAAD2nvAOAAAPo6r+pKo+U1Wfqqo/q6qfr6rH7PA5Lq+qVlUHFm67rqp+Y/FxrbUXtdb+Za+zn3WGG6vqA1V1qqqu2+Z5P7Xwff/7qvr6Ic4GAACrRHgHAIDt+ebW2mOSfGWSv5nkh0Y+z/n4vSTfleR3d/BrDm5+31+R5NeSvPHhov12Lf4hxObnVVX+GwUAgNnzL7UAALADrbV7krwtyZeefV9V7auqH6qqO6vqI1X1mqr6vM2737n544nNBfnfTvLyJH978/MTm8/xqqr6V5s//7tVdXdVfd/m8/1pVb1g4et9QVW9pao+UVXvrqp/dfaC/qyzv6y1dkuSvziP7/vDrbWXJvmRJD92rkBeVS+tqg9tnuk9VfV3Fu77kar6laq6qao+keS6zUv13FBVv5nkviRPrqoXVNX7q+qTVXVHVX3nwnP8QVV988LnF1TVR6vq6Tv9ngAAYCjCOwAA7EBVPSHJs5P8v1vcfd3mxzOTPDnJY5L89OZ9X7v548HW2mNaa+9K8qIk79r8/OA5vuQXJvm8JJckeWGSl1XVRZv3vSzJpzcf8482P4b2hiR/LclTz3H/u5M8PcnnJ/nFJP+uqh65cP81SX4lycEkr9287TuSXJ/kc5PcmeQjSb4pyWOTvCDJT1bVV24+9jVJnrfwfM9O8qettffu4nsCAICuhHcAANieN22u0n8jya8n+V+2eMy1Sf51a+2O1tqnkvxgkuecfUmVHXogyb9orT3QWntrkk8leWpV7U/yrUn+eWvtvtbabUlevYuvs133bv74+Vvd2Vq7qbX25621B1trP5HkEXlopH9Xa+1NrbVTrbXPbN72qtbarZu/5oHW2n9orX2wbfj1JP8xyenl/E1Jnl1Vj938/DuS/ELX7xAAAHZJeAcAgO35+621g621J7bWvmshGi+6OBuL7dPuTHIgyeN38XX/vLX24MLn92VjSX9o87k/tHDf4s93ZOFNVD9VVZcteeglmz9+7BzP832bl4n5+OYfVHxeksc9zBkfcltVXV1Vv11VH9t8jmeffo7W2r1JfjPJt1bVwSRX58xyHgAAJmE3yxsAAOCh7k3yxIXPL0vyYJI/y5lgvajt4msd23zuS5P84eZtTzjfJ9t8A9W/UlWXn+Oh/yAbl4L5wNl3bF7P/fuTXJnk1tbaqao6nqQWv9RWX37hOR6R5FeTPD/Jm1trD1TVm856jlcn+cfZ+O+Zd21edx8AACbD4h0AAPp5XZL/vqqeVFWPycblaH5pc7F+LMmpbFz7/bQ/S3JpVV240y/UWjuZjeut/0hVPaqqviQbsfqcqurCzeutV5ILquqR53qT1C1+7eOr6sVJ/nmSH2ytndriYZ+bjT8MOJbkQFX9s2xcp30nLszG5WmOJXmwqq5O8g1nPeZNSb4yyfdk45rvAAAwKcI7AAD088psXG/8nUn+OMlfJPknSdJauy/JDUl+s6pOVNVXJ/m/k9ya5MNV9dHz+HovzsalXD68+XVfl+Qvlzz+Pyb5TJKvSXLj5s+/dsnjk+REVX06ye9n45Iv/3Vr7ZXneOzbk7wtGwv8O7Px/e/o8jettU8m+adJfjnJ8STfnuTmsx7zmWys4p+UjT98AACASanWdvO3WwEAgKmoqh9L8oWttX809lmGtrmm/+LW2vPGPgsAAJzN4h0AAGaqqr6kqr68NjwjyQuTvHHscw2tqj4/G9/rjWOfBQAAtiK8AwDAfH1uNi618ulsXJrlJ5K8edQTDayq/ttsXL7mba21d459HgAA2IpLzQAAAAAAQEcW7wAAAAAA0JHwDgAAAAAAHR0Y+wA79bjHPa5dfvnlYx8DAAAAAIA19p73vOejrbVDW903u/B++eWX5+jRo2MfAwAAAACANVZVd57rPpeaAQAAAACAjoR3AAAAAADoSHgHAAAAAICOhHcAAAAAAOhIeAcAAAAAgI6EdwAAAAAA6Eh4BwAAAACAjoR3AAAAAADoSHgHAAAAAICOhHcAAAAAAOhIeAcAAAAAgI6EdwAAAAAA6Eh4BwAAAACAjoR3AAAAAADoSHgHAAAAAICOhHcAAAAAAOhIeAcAAAAAgI6EdwAAAAAA6Eh4BwAAAACAjoR3AAAAAADoSHgHAAAAAICOhHcAAAAAAOhIeAcAAAAAgI6EdwAAAAAA6Eh4BwAAAACAjoR3AAAAAADoSHgHAAAAAICOhHcAAAAAAOhIeAcAAAAAgI6EdwAAAAAA6Eh4BwAAAACAjoR3AAAAAADoSHgHAAAAAICOhHcAAAAAAOhIeAcAAAAAgI6EdwAAAAAA6Eh4BwAAAACAjoR3AAAAAADoSHgHAAAAAICOhHcAAAAAAOhIeAcAAAAAgI6EdwAAAAAA6Eh4BwAAAACAjoR3AAAAAADoaLDwXlWvrKqPVNUfnOP+qqp/W1W3V9X7quorhzoLAAAAAABbq6rz+ti/f//YR5+sIRfvr0ryrCX3X53kKZsf1yf52QHPAgAAAADAWarqvH/tqVOnxPdzGCy8t9bemeRjSx5yTZLXtA2/neRgVX3RUOcBAAAAAKCvU6dOjX2ESRrzGu+XJPnQwud3b972Warq+qo6WlVHjx07tieHAwAAAACA8zFmeN/q7zC0rR7YWruxtXa4tXb40KFDAx8LAAAAAADO35jh/e4kT1j4/NIk9450FgAAAAAAdmjfvjET83SN+b/KzUmeXxu+OsnHW2t/OuJ5AAAAAADWSmtbXoRkW/bt25eTJ092PM3qGCy8V9XrkrwryVOr6u6qemFVvaiqXrT5kLcmuSPJ7Un+jyTfNdRZAAAAAADY2j333JMk+bmf+7m01rb9Ibqf24Ghnri19tyHub8l+e6hvj4AAAAAAA/v9Oq9aqu35eR8uAAPAAAAAAB0JLwDAAAAAKwxi/f+hHcAAAAAAOhIeAcAAAAAWGMW7/0J7wAAAAAA0JHwDgAAAACwxize+xPeAQAAAACgI+EdAAAAAGCNWbz3J7wDAAAAAKyx0+GdfoR3AAAAAAAs3jsS3gEAAAAA1pjFe3/COwAAAAAAFu8dCe8AAAAAAGvM4r0/4R0AAAAAAIv3joR3AAAAAIA1dnrxLrz3I7wDAAAAAEBHwjsAAAAAwBqzeO9PeAcAAAAAgI6EdwAAAACANWbx3p/wDgAAAAAAHQnvAAAAAABrzOK9P+EdAAAAAGCNnQ7v9CO8AwAAAABg8d6R8A4AAAAAsMYs3vsT3gEAAAAAsHjvSHgHAAAAAFhjFu/9Ce8AAAAAAFi8dyS8AwAAAACsMYv3/oR3AAAAAAAs3jsS3gEAAAAA1pjFe3/COwAAAAAAFu8dCe8AAAAAAGvM4r0/4R0AAAAAAIv3joR3AAAAAIA1ZvHen/AOAAAAALDGTod3i/d+hHcAAAAAAOhIeAcAAAAAWGMW7/0J7wAAAAAACO8dCe8AAAAAAGvMm6v2J7wDAAAAAGDx3pHwDgAAAACwxize+xPeAQAAAACweO9IeAcAAAAAWGMW7/0J7wAAAAAAWLx3JLwDAAAAAKwxi/f+hHcAAAAAACzeOxLeAQAAAADWmMV7f8I7AAAAAMAaOx3eLd77Ed4BAAAAAKAj4R0AAAAAYI1ZvPcnvAMAAAAAQEfCOwAAAADAGrN47094BwAAAACAjoR3AAAAAIA1ZvHen/AOAAAAAAAdCe8AAAAAAGvM4r0/4R0AAAAAADoS3gEAAAAAZuSqq65KVXX7+Jqv+ZokydVXX52qymtf+9qRv8P5E94BAAAAAGbiqquuyi233DLo13je854nvu+S8A4AAAAAMBNDR/fTXvKSl+zJ11lVwjsAAAAAAA9x1113jX2EWRPeAQAAAAB4iMsuu2zsI8ya8A4AAAAAMBNXXnnlnnydG264YU++zqoS3gEAAAAAZuLIkSODx/ebbrop11577aBfY9UJ7wAAAAAAM3LkyJG87nWvS5Lcdtttaa11/RDdd094BwAAAACYmdZakqSqRj4JWxHeAQAAAACgI+EdAAAAAGBmLN6nTXgHAAAAAICOhHcAAAAAgJmxeJ824R0AAAAAADoS3gEAAAAAZsbifdqEdwAAAAAA6Eh4BwAAAACYGYv3aRPeAQAAAACgI+EdAAAAAGBmLN6nTXgHAAAAAJgp4X2ahHcAAAAAgJk5vXhnmoR3AAAAAICZsnifJuEdAAAAAGBmLN6nTXgHAAAAAJgZb646bcI7AAAAAAB0JLwDAAAAAMyMxfu0Ce8AAAAAANCR8A4AAAAAMDMW79MmvAMAAAAAQEfCOwAAAADAzFi8T5vwDgAAAAAAHQnvAAAAAAAzY/E+bcI7AAAAAAB0JLwDAAAAAMyMxfu0Ce8AAAAAANCR8A4AAAAAMDMW79MmvAMAAAAAQEfCOwAAAADAzFi8T5vwDgAAAAAAHQnvAAAAAAAzY/E+bcI7AAAAAAB0JLwDAAAAAMyMxfu0Ce8AAAAAADMlvE+T8A4AAAAAMDOnF+9Mk/AOAAAAADBTFu/TJLwDAAAAAMyMxfu0Ce8AAAAAADPjzVWnTXgHAAAAAICOhHcAAAAAgJmxeJ824R0AAAAAADoS3gEAAAAAZsbifdqEdwAAAAAA6Eh4BwAAAACYGYv3aRPeAQAAAACgI+EdAAAAAGBmLN6nTXgHAAAAAICOhHcAAAAAgJmxeJ824R0AAAAAADoS3gEAAAAAZsbifdqEdwAAAAAA6Eh4BwAAAACYGYv3aRPeAQAAAACgI+EdAAAAAGBmLN6nTXgHAAAAAICOhHcAAAAAgJmxeJ824R0AAAAAYKaE92k6MPYBAAAAAADm5LWvfW2e97znjX2MJMmjH/3oJGcW8EyDxTsAAAAAwDZNKbovsnyfFuEdAAAAAGCbXvKSl4x9BGZAeAcAAAAA2Ka77rpr7CMwA8I7AAAAAMA2XXbZZWMfgRkQ3gEAAAAAtumGG24Y+wjMgPAOAAAAALBN1157bW666aaxj/FZWmtjH4EFwjsAAAAAwA5ce+21+Z3f+Z0kyVve8pa01kb/YFqEdwAAAACAHTodu6tq5JMwRcI7AAAAAAB0JLwDAAAAAOyQxTvLCO8AAAAAAOdJeGcrwjsAAAAAwA55Q1OWEd4BAAAAAM6TxTtbEd4BAAAAAHbI4p1lhHcAAAAAgPNk8c5WhHcAAAAAgB2yeGcZ4R0AAAAA4DxZvLMV4R0AAAAAYIcs3llGeAcAAAAAOE8W72xFeAcAAAAA2CGLd5YR3gEAAAAAduh0eLd4ZyvCOwAAAAAAdCS8AwAAAADskMU7ywjvAAAAAADQkfAOAAAAALBDFu8sI7wDAAAAAJwn4Z2tCO8AAAAAADt0evEOWxHeAQAAAADOk8U7WxHeAQAAAAB2yOKdZYR3AAAAAIDzZPHOVoR3AAAAAIAdsnhnGeEdAAAAAOA8WbyzFeEdAAAAAGCHLN5ZRngHAAAAADhPFu9sRXgHAAAAANghi3eWEd4BAAAAAM6TxTtbEd4BAAAAAHbI4p1lhHcAAAAAgPNk8c5WhHcAAAAAgB06vXgX3tmK8A4AAAAAAB0J7wAAAAAAO2TxzjLCOwAAAAAAdCS8AwAAAADskMU7ywjvAAAAAAA7dDq8w1aEdwAAAACA82TxzlaEdwAAAACAHbJ4ZxnhHQAAAADgPFm8sxXhHQAAAABghyzeWUZ4BwAAAAA4TxbvbEV4BwAAAADYIYt3lhHeAQAAAADOk8U7WxHeAQAAAAB26PTiXXhnK8I7AAAAAAB0JLwDAAAAAOyQxTvLCO8AAAAAANCR8A4AAAAAsEMW7ywjvAMAAAAAQEfCOwAAAADADlm8s4zwDgAAAAAAHQnvAAAAAAA7ZPHOMsI7AAAAAAB0JLwDAAAAAOyQxTvLCO8AAAAAANCR8A4AAAAAsEMW7ywzaHivqmdV1Qeq6vaq+oEt7v+8qnpLVf1eVd1aVS8Y8jwAAAAAAD0J72zlwFBPXFX7k7wsydcnuTvJu6vq5tbabQsP++4kt7XWvrmqDiX5QFW9trV2/1DnAgAAAADmY+ph+8u+7MuSnFnAQzLs4v0ZSW5vrd2xGdJfn+Sasx7Tknxubfy/5zFJPpbkwQHPBAAAAADMxNSj+6I5nZXhDRneL0nyoYXP7968bdFPJ/kbSe5N8vtJvqe1dmrAMwEAAAAAwKCGDO9b/RHP2X/f4huTvDfJxUmenuSnq+qxn/VEVddX1dGqOnrs2LHe5wQAAAAAgG6GDO93J3nCwueXZmPZvugFSd7QNtye5I+TfMnZT9Rau7G1dri1dvjQoUODHRgAAAAAAHZryPD+7iRPqaonVdWFSZ6T5OazHnNXkiuTpKoen+SpSe4Y8EwAAAAAADCoA0M9cWvtwap6cZK3J9mf5JWttVur6kWb9788yb9M8qqq+v1sXJrm+1trHx3qTAAAAADAfLTWZvOmpa2dfZVt1tlg4T1JWmtvTfLWs257+cLP703yDUOeAQAAAACYr2PHjuXQoUP5qZ/6qbz4xS8e+ziwLUNeagYAAAAAYFdOL8nnsnyHRHgHAAAAAICuhHcAAAAAYLIs3pkj4R0AAAAAADoS3gEAAACAybJ4Z46EdwAAAAAA6Eh4BwAAAAAmy+KdORLeAQAAAIDJE96ZE+EdAAAAAJis04t3mBPhHQAAAACYPIt35kR4BwAAAAAmy+KdORLeAQAAAIDJ8uaqzJHwDgAAAAAAHQnvAAAAAMBkWbwzR8I7AAAAAAB0JLwDAAAAAJNl8c4cCe8AAAAAANCR8A4AAAAATJbFO3MkvAMAAAAAQEfCOwAAAAAwWRbvzJHwDgAAAAAAHQnvAAAAAMBkWbwzR8I7AAAAAAB0JLwDAAAAAJNl8c4cCe8AAAAAANCR8A4AAAAATJbFO3MkvAMAAAAAQEfCOwAAAAAwWRbvzJHwDgAAAAAAHQnvAAAAAMBkWbwzR8I7AAAAADB5wjtzIrwDAAAAAJN1evEOcyK8AwAAAACTZ/HOnAjvAAAAAMBkWbwzR8I7AAAAADBZ3lyVORLeAQAAAACgI+EdAAAAAJgsi3fmSHgHAAAAAICOhHcAAAAAYLIs3pkj4R0AAAAAADoS3gEAAACAybJ4Z46EdwAAAAAA6Eh4BwAAAAAmy+KdORLeAQAAAACgI+EdAAAAAJgsi3fmSHgHAAAAAICOhHcAAAAAYLIs3pkj4R0AAAAAADoS3gEAAACAybJ4Z46EdwAAAAAA6Eh4BwAAAAAmy+KdORLeAQAAAACgI+EdAAAAAJgsi3fmSHgHAAAAACZPeGdOhHcAAAAAYLJOL95hToR3AAAAAGDyLN6ZE+EdAAAAAJgsi3fmSHgHAAAAACbLm6syRwfGPgAAAAAAsHf279+fU6dOjX2MHfuWb/mWJMlNN92Ua6+9duTTwHIW7wAAAACwJuYa3Rc973nPy2tf+9qxjwFLCe8AAAAAsCbmHt1Pe8lLXjL2EWAp4R0AAAAAmJW77rpr7CPAUsI7AAAAADArl1122dhHgKWEdwAAAABYE/v2rUYOvOGGG8Y+Aiy1Gv9PAwAAAAAe1smTJ2cf32+66aZce+21Yx8Dljow9gEAAAAAgL1z8uTJfO3Xfm3279+fd7zjHWMfB1bSvP94CwAAAADYsdZaqmrsY8DKEt4BAAAAAKAj4R0AAAAA1ozFOwxLeAcAAACANSS8w3CEdwAAAABYM621sY8AK014BwAAAIA1ZPEOwxHeAQAAAGDNWLzDsIR3AAAAAFgz3lwVhiW8AwAAAABAR8I7AAAAAKwZi3cYlvAOAAAAAAAdCe8AAAAAsGYs3mFYwjsAAAAAAHQkvAMAAADAmrF4h2EJ7wAAAAAA0JHwDgAAAABrxuIdhiW8AwAAAABAR8I7AAAAAKwZi3cYlvAOAAAAAAAdCe8AAAAAsGYs3mFYwjsAAAAAAHQkvAMAAADAmrF4h2EJ7wAAAAAA0JHwDgAAAABrxuIdhiW8AwAAAABAR8I7AAAAAKwZi3cYlvAOAAAAAGtIeIfhCO8AAAAAsGZaa2MfAVaa8A4AAAAAa8jiHYYjvAMAAADAmrF4h2EJ7wAAAACwZry5KgxLeAcAAAAAgI6EdwAAAABYMxbvMCzhHQAAAAAAOhLeAQAAAGDNWLzDsIR3AAAAAADoSHgHAAAAgDVj8Q7DEt4BAAAAAKAj4R0AAAAA1ozFOwxLeAcAAAAAgI6EdwAAAABYMxbvMCzhHQAAAAAAOhLeAQAAAGDNWLzDsIR3AAAAAADoSHgHAAAAgDVj8Q7DEt4BAAAAAKAj4R0AAAAA1ozFOwxLeAcAAAAAgI6EdwAAAABYMxbvMCzhHQAAAADWkPAOwxHeAQAAAGDNtNbGPgKsNOEdAAAAANaQxTsMR3gHAAAAgDVj8Q7DEt4BAAAAYM14c1UYlvAOAAAAAAAdCe8AAAAAsGYs3mFYwjsAAAAAAHQkvAMAAADAOVx11VWpqpX7uPPOO/Oa17wmVZWLLrpo7P+ZYeUI7wAAAACwhauuuiq33HLL2McY3IkTJ8R36Ex4BwAAAIAtrEN0P+3EiRNjHwFWivAOAAAAAAAdCe8AAAAAANCR8A4AAAAAW7jyyivHPsKeOXjw4NhHgJUivAMAAADAFo4cObIW8f3gwYM5fvz42MeAlSK8AwAAAMA5HDlyJL/4i7+YJHn/+9+f1trKfYju0J/wDgAAAAAAHQnvAAAAALBEay1JUlUjnwSYC+EdAAAAALZBeAe2S3gHAAAAgCVOL94Btkt4BwAAAIBtsHgHtkt4BwAAAIAlLN6BnRLeAQAAAGAJb64K7JTwDgAAAAAAHQnvAAAAALCExTuwU8I7AAAAAAB0JLwDAAAAwBIW78BOCe8AAAAAANCR8A4AAAAAS1i8AzslvAMAAAAAQEfCOwAAAAAsYfEO7JTwDgAAAAAAHQnvAAAAALCExTuwU8I7AAAAAAB0JLwDAAAAwBIW78BOCe8AAAAAANCR8A4AAAAAS1i8AzslvAMAAAAAQEfCOwAAAAAsYfEO7JTwDgAAAAAAHQnvAAAAALCExTuwU8I7AAAAAGyD8A5sl/AOAAAAAEucXrwDbJfwDgAAAADbYPEObJfwDgAAAABLWLwDOyW8AwAAAMAS3lwV2CnhHQAAAAAAOhLeAQAAAGAJi3dgp4R3AAAAAADoSHgHAAAAgCUs3oGdEt4BAAAAAKAj4R0AAAAAlrB4B3ZKeAcAAAAAgI6EdwAAAABYwuId2CnhHQAAAAAAOhLeAQAAAGAJi3dgp4R3AAAAAADoSHgHAAAAgCUs3oGdEt4BAAAAAKAj4R0AAAAAlrB4B3ZKeAcAAAAAgI6EdwAAAABYwuId2CnhHQAAAAAAOhLeAQAAAGAJi3dgp4R3AAAAANgG4R3YLuEdAAAAAJY4vXgH2C7hHQAAAAC2weId2C7hHQAAAACWsHgHdkp4BwAAAIAlvLkqsFPCOwAAAAAAdCS8AwAAAMASFu/ATgnvAAAAAADQkfAOAAAAAEtYvAM7JbwDAAAAAEBHB4Z88qp6VpKXJtmf5BWttR/d4jF/N8m/SXJBko+21r5uyDMBAAAA0Ne6LMEf/ehHJzmzgAc4l8HCe1XtT/KyJF+f5O4k766qm1trty085mCSn0nyrNbaXVX114Y6DwAAAAD9rUt0X1RV4juw1JCXmnlGkttba3e01u5P8vok15z1mG9P8obW2l1J0lr7yIDnAQAAAACAwQ0Z3i9J8qGFz+/evG3RFye5qKr+U1W9p6qev9UTVdX1VXW0qo4eO3ZsoOMCAAAAAMDuDRnet/p7Rmf/HZwDSb4qyd9L8o1JfriqvvizflFrN7bWDrfWDh86dKj/SQEAAAAAoJMh31z17iRPWPj80iT3bvGYj7bWPp3k01X1ziRfkeQPBzwXAAAAAAAMZsjF+7uTPKWqnlRVFyZ5TpKbz3rMm5P8nao6UFWPSvK3krx/wDMBAAAA0NE6vsnoOn7PwM4MFt5baw8meXGSt2cjpv9ya+3WqnpRVb1o8zHvT/J/JXlfkt9J8orW2h8MdSYAAAAA+mut5fGPf3yuv/76tNZW/gPg4Qx5qZm01t6a5K1n3fbysz7/8SQ/PuQ5AAAAABhe1VZv+Qewfoa81AwAAAAAa8ISHOAM4R0AAACALizeATYI7wAAAADsmsU7wBnCOwAAAAC71lqzeAfYJLwDAAAAAEBHwjsAAAAAu2bxDnCG8A4AAAAAAB0J7wAAAADsmsU7wBnCOwAAAAAAdCS8AwAAALBrFu8AZwjvAAAAAADQkfAOAAAAwK5ZvAOcIbwDAAAAAEBHwjsAAAAAu2bxDnCG8A4AAAAAAB0J7wAAAADsmsU7wBnCOwAAAAAAdCS8AwAAALBrFu8AZwjvAAAAAADQkfAOAAAAwK5ZvAOcIbwDAAAAAEBHwjsAAAAAu2bxDnCG8A4AAABAF8I7wAbhHQAAAIBda62NfQSAyRDeAQAAAOjC4h1gg/AOAAAAwK5ZvAOcIbwDAAAAsGveXBXgDOEdAAAAAAA6Et4BAAAA2DWLd4AzhHcAAAAAAOhIeAcAAABg1yzeAc4Q3gEAAAAAoCPhHQAAAIBds3gHOEN4BwAAAACAjoR3AAAAAHbN4h3gDOEdAAAAAAA6Et4BAAAA2DWLd4AzhHcAAAAAAOhIeAcAAABg1yzeAc4Q3gEAAAAAoCPhHQAAAIBds3gHOEN4BwAAAACAjoR3AAAAAHbN4h3gDOEdAAAAAAA6Et4BAAAA6MLiHWCD8A4AAABAF8I7wAbhHQAAAIBdaa2NfQSASRHeAQAAAOjC4h1gg/AOAAAAwK5YvAM8lPAOAAAAwK6cDu8W7wAbhHcAAAAAAOhIeAcAAABgVyzeAR5KeAcAAAAAgI6EdwAAAAB2xeId4KGEdwAAAAAA6Eh4BwAAAGBXLN4BHkp4BwAAAACAjur0n0jOxeHDh9vRo0fHPgYAAACwpi666KKcOHFi7GNM2hVXXJFbb7117GMADKqq3tNaO7zVfRbvAAAAANskum/Pbbfdlqc97WljHwNgNMI7AAAAwDaJ7tt32223jX0EgNEI7wAAAAAA0JHwDgAAAAAAHQnvAAAAANt08ODBsY8wG1dcccXYRwAYjfAOAAAAsE3Hjx8X37fhiiuuyK233jr2MQBGc2DsAwAAAADMyfHjx/O93/u9+fmf//l8/OMfH/s4AEyQxTsAAADAeaiqsY8AwEQJ7wAAAAA71Fob+wgATJjwDgAAALBDrTWLdwDOSXgHAAAAAICOhHcAAACAHbJ4B2AZ4R0AAAAAADp62PBeVf+wqv6oqj5eVZ+oqk9W1Sf24nAAAAAAU2TxDsAyB7bxmP8tyTe31t4/9GEAAAAAAGDutnOpmT8T3QEAAADOsHgHYJntLN6PVtUvJXlTkr88fWNr7Q1DHQoAAAAAAOZqO+H9sUnuS/INC7e1JMI7AAAAsJYs3gFY5mHDe2vtBXtxEAAAAAAAWAUPG96r6pFJXpjkaUkeefr21tp/M+C5AAAAACbL4h2AZbbz5qq/kOQLk3xjkl9PcmmSTw55KAAAAAAAmKvthPe/3lr74SSfbq29OsnfS/Jlwx4LAAAAYLos3gFYZjvh/YHNH09U1Zcm+bwklw92IgAAAAAAmLGHvcZ7khur6qIkP5zk5iSPSfLPBj0VAAAAwIRZvAOwzMOG99baKzZ/+utJnjzscQAAAAAAYN4e9lIzVfX4qvo/q+ptm59fUVUvHP5oAAAAANNk8Q7AMtu5xvurkrw9ycWbn/9hku8d6DwAAAAAsyC8A3Au2wnvj2ut/XKSU0nSWnswyclBTwUAAAAwYa21sY8AwIRtJ7x/uqq+IElLkqr66iQfH/RUAAAAABNn8Q7AuTzsm6sm+R+S3JzkP6uq30xyKMl/NeipAAAAACbM4h2AZR42vLfWfreqvi7JU5NUkg+01h4Y/GQAAAAAE+XNVQFY5pzhvar+4Tnu+uKqSmvtDQOdCQAAAAAAZmvZ4v1Xkrx38yPZWLuf1pII7wAAAMBasngHYJll4f1bk3xbki9P8uYkr2ut3b4npwIAAAAAgJnad647WmtvbK09J8nXJflgkp+oqt/YvN47AAAAwNqyeAdgmXOG9wV/keTjST6R5NFJHjnoiQAAAAAAYMaWvbnqM5M8N8kzkhxJ8tLW2tG9OhgAAADAVFm8A7DMsmu835LkfUl+I8kjkjy/qp5/+s7W2j8d+GwAAAAAADA7y8L7C/bsFAAAAAAzYvEOwDLnDO+ttVfv5UEAAAAAAGAVLFu8J0mq6ouT/I9JLl98fGvtvxzuWAAAAADTZfEOwDIPG96T/LskL0/yiiQnhz0OAAAAAADM23bC+4OttZ8d/CQAAAAAM2HxDsAy5wzvVfX5mz99S1V9V5I3JvnL0/e31j428NkAAAAAAGB2li3e35OkJTn9x7f/08J9LcmThzoUAAAAwJRZvAOwzDnDe2vtSUlSVY9srf3F4n1V9cihDwYAAAAAAHO0bxuP+a1t3gYAAACwFizeAVhm2TXevzDJJUk+p6r+85y55MxjkzxqD84GAAAAAACzs+wa79+Y5Loklyb51wu3fzLJ/zzgmQAAAAAmzeIdgGWWXeP91UleXVXf2lr71T08EwAAAMDkCe8AnMuyxftpX1pVTzv7xtbavxjgPAAAAACT11ob+wgATNh2wvunFn7+yCTflOT9wxwHAAAAYB4s3gE4l4cN7621n1j8vKr+9yQ3D3YiAAAAgImzeAdgmX3n8WseleTJvQ8CAAAAMBfeXBWAZR528V5Vv5/k9B/j7k9yKInruwMAAAAAwBa2c433b1r4+YNJ/qy19uBA5wEAAACYPIt3AJZZGt6ral+S/9Ba+9I9Og8AAAAAAMza0mu8t9ZOJfm9qrpsj84DAAAAMHkW7wAss51LzXxRklur6neSfPr0ja21bxnsVAAAAAAAMFPbCe+PyUOv815JfmyY4wAAAABMn8U7AMtsJ7wfaK39+uINVfU5A50HAAAAAABm7Zzhvar+uyTfleTJVfW+hbs+N8lvDn0wAAAAgKmyeAdgmWWL919M8rYk/2uSH1i4/ZOttY8NeioAAAAAAJipc4b31trHk3w8yXP37jgAAAAA02fxDsAy+8Y+AAAAAAAArJLtvLkqAAAAsOKuuuqq3HLLLWMfY3aqKvv27cvJkyfHPgoAE2LxDgAAAGtOdN+dU6dOZf/+/WMfA4AJEd4BAABgzYnuu3fq1KmxjwDAhAjvAAAAAADQkfAOAAAAAAAdCe8AAACw5q688sqxjzB7+/ZJLACc4Z8KAAAAsOaOHDkivu/Cvn37cvLkybGPAcCECO8AAABAjhw5kl/4hV9IkvzRH/1RWms+tvkhugNwNuEdAAAASJK01pIkVTXySQBg3oR3AAAAAADoSHgHAAAAkli8A0AvwjsAAAAAAHQkvAMAAABJLN4BoBfhHQAAAAAAOhLeAQAAgCQW7wDQi/AOAAAAAAAdCe8AAABAEot3AOhFeAcAAAAAgI6EdwAAACCJxTsA9CK8AwAAAABAR8I7AAAAkMTiHQB6Ed4BAAAAAKAj4R0AAABIYvEOAL0I7wAAAAAA0JHwDgAAACSxeAeAXoR3AAAA4CGEdwDYHeEdAAAASHJm8Q4A7I7wDgAAADyExTsA7I7wDgAAACSxeAeAXoR3AAAAIIk3VwWAXoR3AAAAAADoSHgHAAAAkli8A0AvwjsAAAAAAHQkvAMAAABJLN4BoBfhHQAAAAAAOhLeAQAAgCQW7wDQi/AOAAAAAAAdCe8AAABAEot3AOhFeAcAAAAAgI6EdwAAACCJxTsA9CK8AwAAAABAR8I7AAAAkMTiHQB6Ed4BAAAAAKAj4R0AAABIYvEOAL0I7wAAAAAA0JHwDgAAACSxeAeAXoR3AAAAAADoSHgHAAAAkli8A0AvwjsAAADwEMI7AOyO8A4AAAAkObN4BwB2R3gHAAAAHsLiHQB2R3gHAAAAkli8A0AvwjsAAACQxJurAkAvwjsAAAAAAHQkvAMAAABJLN4BoBfhHQAAAAAAOhLeAQAAgCQW7wDQi/AOAAAAAAAdCe8AAABAEot3AOhFeAcAAAAAgI6EdwAAACCJxTsA9CK8AwAAAABAR8I7AAAAkMTiHQB6Ed4BAAAAAKAj4R0AAABIYvEOAL0I7wAAAAAA0NGBIZ+8qp6V5KVJ9id5RWvtR8/xuL+Z5LeTfFtr7VeGPBMAAACrYf/+/Tl16tTYx1hJj3jEI5KcWcADADsz2OK9qvYneVmSq5NckeS5VXXFOR73Y0nePtRZAAAAWC2i+95wyRkAOD9DXmrmGUlub63d0Vq7P8nrk1yzxeP+SZJfTfKRAc8CAADAChHdAYApGzK8X5LkQwuf371521+pqkuS/IMkL1/2RFV1fVUdraqjx44d635QAAAAAADoZcjwvtXfRzv74nD/Jsn3t9ZOLnui1tqNrbXDrbXDhw4d6nU+AAAAAADobsg3V707yRMWPr80yb1nPeZwktdvXjPucUmeXVUPttbeNOC5AAAAmLl9+/a53AwAMFlDhvd3J3lKVT0pyT1JnpPk2xcf0Fp70umfV9Wrkvx70R0AAICHc/LkSW+wugdaO/svrgMA2zFYeG+tPVhVL07y9iT7k7yytXZrVb1o8/6l13UHAACAZU6ePJlnPvOZOXnyZN75zneOfRwAgL8y5OI9rbW3JnnrWbdtGdxba9cNeRYAAABWT2stm5cvBQCYjCHfXBUAAAAAANaO8A4AAMBsWbwDAFMkvAMAAAAAQEfCOwAAALNl8Q4ATJHwDgAAAAAAHQnvAAAAzJbFOwAwRcI7AAAAsya8AwBTI7wDAAAwW621sY8AAPBZhHcAAABmzeIdAJga4R0AAIDZsngHAKZIeAcAAGDWLN4BgKkR3gEAAJgti3cAYIqEdwAAAGbN4h0AmBrhHQAAgNmyeAcApkh4BwAAYNYs3gGAqRHeAQAAmC2LdwBgioR3AAAAZs3iHQCYGuEdAACA2bJ4BwCmSHgHAABgtlprFu8AwOQI7wAAAMya8A4ATI3wDgAAwGy51AwAMEXCOwAAALNm8Q4ATI3wDgAAwGxZvAMAUyS8AwAAMGsW7wDA1AjvAAAAzJbFOwAwRcI7AAAAs2bxDgBMjfAOAADAbFm8AwBTJLwDAAAwaxbvAMDUCO8AAADMlsU7ADBFwjsAAACzZvEOAEyN8A4AAMBsWbwDAFMkvAMAADBrFu8AwNQI7wAAAMyWxTsAMEXCOwAAALNm8Q4ATI3wDgAAwGy11oR3AGByhHcAAAAAAOhIeAcAAGC2LN4BgCkS3gEAAAAAoCPhHQAAgNmyeAcApkh4BwAAAACAjoR3AAAAZsviHQCYIuEdAACA2WqtjX0EAIDPIrwDAAAwaxbvAMDUCO8AAADMlsU7ADBFwjsAAACzZvEOAEyN8A4AAMBsWbwDAFMkvAMAADBrFu8AwNQI7wAAAMxWa014BwAmR3gHAAAAAICOhHcAAABmy+IdAJgi4R0AAAAAADoS3gEAAJgti3cAYIqEdwAAAAAA6Eh4BwAAYLYs3gGAKRLeAQAAAACgI+EdAACA2bJ4BwCmSHgHAAAAAICODox9AAAAgKl41KMelc985jNjH4MduuOOO3LTTTfl4MGDOX78+NjHAQCweAcAAEhE91Vw4sSJXHTRRWMfAwBAeAcAAEgiuq+IEydOjH0EAADhHQAAAAAAehLeAQAAAACgI+EdAAAgyed8zueMfQQ6OHjw4NhHAAAQ3gEAAJLkvvvuE99n7uDBgzl+/PjYxwAAyIGxDwAAADAV9913X6677rq84x3vyJ133jn2cQAAmCmLdwAAgAWttVTV2McAAGDGhHcAAAAAAOhIeAcAAFhg8Q4AwG4J7wAAAAAA0JHwDgAAsMDiHQCA3RLeAQAAAACgI+EdAABggcU7AAC7JbwDAAAAAEBHwjsAAMACi3cAAHZLeAcAAAAAgI6EdwAAgAUW7wAA7JbwDgAAAAAAHQnvAAAACyzeAQDYLeEdAADgLMI7AAC7IbwDAAAsaK2NfQQAAGZOeAcAADiLxTsAALshvAMAACyweAcAYLeEdwAAgAXeXBUAgN0S3gEAAAAAoCPhHQAAYIHFOwAAuyW8AwAAAABAR8I7AADAAot3AAB2S3gHAAAAAICOhHcAAIAFFu8AAOyW8A4AAAAAAB0J7wAAAAss3gEA2C3hHQAAAAAAOhLeAQAAFli8AwCwW8I7AAAAAAB0JLwDAAAssHgHAGC3hHcAAAAAAOhIeAcAAFhg8Q4AwG4J7wAAAAAA0JHwDgAAsMDiHQCA3RLeAQAAziK8AwCwG8I7AADAgtba2EcAAGDmhHcAAICzWLwDALAbwjsAAMACi3cAAHZLeAcAAFjgzVUBANgt4R0AAAAAADoS3gEAABZYvAMAsFvCOwAAAAAAdCS8AwAALLB4BwBgt4R3AAAAAADoSHgHAABYYPEOAMBuCe8AAAAAANCR8A4AALDA4h0AgN0S3gEAAAAAoCPhHQAAYIHFOwAAuyW8AwAAAABAR8I7AADAAot3AAB2S3gHAAAAAICOhHcAAIAFFu8AAOyW8A4AAAAAAB0J7wAAAAss3gEA2C3hHQAAAAAAOhLeAQAAFli8AwCwW8I7AADAWYR3AAB248DYBwAAgL121VVX5ZZbbhn7GExcVeWCCy7I/fffP/ZRAACYGYt3AADWiujOTjzwwAO58MILxz4GAAAzI7wDALBWRHd26oEHHhj7CAAAzIzwDgAAAAAAHQnvAAAAAADQkfAOAMBaufLKK8c+AjNzwQUXjH0EAABmRngHAGCtHDlyRHxn2y644ILcf//9Yx8DAICZOTD2AQAAYK8dOXIkr371q3Pdddflgx/8YJ785CePfSQAAGCFWLwDALCWWmtJkqoa+SQAAMCqEd4BAAAAAKAj4R0AgLVk8Q4AAAxFeAcAAAAAgI6EdwAA1pLFOwAAMBThHQAAAAAAOhLeAQBYSxbvAADAUIR3AAAAAADoSHgHAGAtWbwDAABDEd4BAFhrwjsAANCb8A4AwFo6vXgHAADoTXgHAGCtWbwDAAC9Ce8AAKwli3cAAGAowjsAAGvJm6sCAABDEd4BAAAAAKAj4R0AgLVk8Q4AAAxFeAcAAAAAgI6EdwAA1pLFOwAAMBThHQAAAAAAOhLeAQBYSxbvAADAUIR3AAAAAADoSHgHAGAtWbwDAABDEd4BAAAAAKAj4R0AgLVk8Q4AAAxFeAcAAAAAgI6EdwAA1pLFOwAAMBThHQAAAAAAOhLeAQBYSxbvAADAUIR3AAAAAADoSHgHAGAtWbwDAABDEd4BAAAAAKAj4R0AgLVk8Q4AAAxFeAcAYK0J7wAAQG/COwAAa+n04h0AAKA34R0AgLVm8Q4AAPQmvAMAsJYs3gEAgKEI7wAArCVvrgoAAAxFeAcAAAAAgI6EdwAA1pLFOwAAMBThHQAAAAAAOhLeAQBYSxbvAADAUIR3AAAAAADoSHgHAGAtWbwDAABDEd4BAAAAAKAj4R0AgLVk8Q4AAAxFeAcAAAAAgI6EdwAA1pLFOwAAMBThHQAAAAAAOhLeAQBYSxbvAADAUIR3AAAAAADoSHgHAGAtWbwDAABDEd4BAAAAAKAj4R0AgLVk8Q4AAAxFeAcAAAAAgI6EdwAA1pLFOwAAMBThHQCAtSa8AwAAvQnvAACspdOLdwAAgN4GDe9V9ayq+kBV3V5VP7DF/ddW1fs2P36rqr5iyPMAAMDZLN4BAIDeDgz1xFW1P8nLknx9kruTvLuqbm6t3bbwsD9O8nWtteNVdXWSG5P8raHOBABMj+jJ2A4c2PhXYgt4AACglyEX789Icntr7Y7W2v1JXp/kmsUHtNZ+q7V2fPPT305y6YDnAQAmRnRnSrweAQCAXoYM75ck+dDC53dv3nYuL0zytgHPAwAAAAAAgxvsUjNJtpoMbfn3d6vqmdkI7//FOe6/Psn1SXLZZZf1Oh8AAAAAAHQ35OL97iRPWPj80iT3nv2gqvryJK9Ick1r7c+3eqLW2o2ttcOttcOHDh0a5LAAAAAAANDDkOH93UmeUlVPqqoLkzwnyc2LD6iqy5K8Icl3tNb+cMCzAAAAAADAnhjsUjOttQer6sVJ3p5kf5JXttZuraoXbd7/8iT/LMkXJPmZzTezerC1dnioMwEA09Ja84aWTEZrW14VEQAAYMdqbv+Bcfjw4Xb06NGxjwEAdPT0pz89T3ziE/PmN7957KMAAADAtlTVe841JB/yUjMAANti+Q4AAMAqEd4BAAAAAKAj4R0AGJ3FOwAAAKtEeAcAJkF4BwAAYFUI7wDA6Ob2Zu8AAACwjPAOAEyCxTsAAACrQngHAEZn8Q4AAMAqEd4BgEmweAcAAGBVCO8AwOgs3gEAAFglwjsAMAkW7wAAAKwK4R0AGJ3FOwAAAKtEeAcARtdas3gHAABgZQjvAAAAAADQkfAOAIzO4h0AAIBVIrwDAAAAAEBHwjsAMDqLdwAAAFaJ8A4ATILwDgAAwKoQ3gGA0bXWxj4CAAAAdCO8AwCTYPEOAADAqhDeAYDRWbwDAACwSoR3AGASLN4BAABYFcI7ADA6i3cAAABWifAOAEyCxTsAAACrQngHAEZn8Q4AAMAqEd4BgEmweAcAAGBVCO8AwOgs3gEAAFglwjsAMAkW7wAAAKwK4R0AGJ3FOwAAAKtEeAcAJsHiHQAAgFUhvAMAo2utCe8AAACsDOEdAAAAAAA6Et4BgNFZvAMAALBKhHcAAAAAAOhIeAcARmfxDgAAwCoR3gGA0bXWxj4CAAAAdCO8AwCTYPEOAADAqhDeAYDRWbwDAACwSoR3AGASLN4BAABYFcI7ADA6i3cAAABWifAOAEyCxTsAAACrQngHAEZn8Q4AAMAqEd4BgEmweAcAAGBVCO8AwOhaa8I7AAAAK0N4BwAAAACAjoR3AGB0Fu8AAACsEuEdAAAAAAA6Et4BgNFZvAMAALBKhHcAAAAAAOhIeAcARmfxDgAAwCoR3gEAAAAAoCPhHQAYncU7AAAAq0R4BwAAAACAjoR3AGB0Fu8AAACsEuEdAAAAAAA6Et4BgNFZvAMAALBKhHcAAAAAAOhIeAcARmfxDgAAwCoR3gGA0QnvAAAArJJqrY19hh05fPhwO3r06NjHAJiFiy66KCdOnBj7GLAjF198ce65556xjwEAAABLVdV7WmuHt7rP4h1gRYnuzNW9996bSy65ZOxjAAAAwHkT3gFWlOjOnN17771jHwEAAADOm/AOAAAAAAAdCe8AAAAAANCR8A6wog4ePDj2EeC8XXzxxWMfAQAAAM6b8A6woo4fPy6+M0sXX3xx7rnnnrGPAQAAAOftwNgHAGA4x48fz3d+53fmzW9+cz784Q+PfRwAAACAtWDxDrDiWmupqrGPAQAAALA2hHcAAAAAAOhIeAdYcRbvAAAAAHtLeAcAAAAAgI6Ed4AVZ/EOAAAAsLeEd4A1ILwDAAAA7B3hHWDFtdbGPgIAAADAWhHeAdaAxTsAAADA3hHeAVacxTsAAADA3hLeAVacN1cFAAAA2FvCOwAAAAAAdCS8A6w4i3cAAACAvSW8AwAAAABAR8I7wIqzeAcAAADYW8I7AAAAAAB0JLwDrDiLdwAAAIC9JbwDAAAAAEBHwjvAirN4BwAAANhbwjsAAAAAAHQkvAOsOIt3AAAAgL0lvAMAAAAAQEfCO8CKs3gHAAAA2FvCOwAAAAAAdCS8A6w4i3cAAACAvSW8AwAAAABAR8I7wIqzeAcAAADYW8I7AAAAAAB0JLwDrDiLdwAAAIC9JbwDrAHhHQAAAGDvCO8AK661NvYRAAAAANaK8A6wBizeAQAAAPaO8A6w4izeAQAAAPaW8A6w4ry5KgAAAMDeEt4BAAAAAKAj4R1gxVm8AwAAAOwt4R0AAAAAADoS3gFWnMU7AAAAwN4S3gEAAAAAoCPhHWDFWbwDAAAA7C3hHQAAAAAAOhLeAVacxTsAAADA3hLeAQAAAACgI+EdYMVZvAMAAADsLeEdAAAAAAA6Et4BVpzFOwAAAMDeEt4BAAAAAKAj4R1gxVm8AwAAAOwt4R0AAAAAADoS3gFWnMU7AAAAwN4S3gHWgPAOAAAAsHeEd4AV11ob+wgAAAAAa0V4B1gDFu8AAAAAe0d4B1hxFu8AAAAAe0t4B1hx3lwVAAAAYG8J7wAAAAAA0JHwDrDiLN4BAAAA9pbwDgAAAAAAHR0Y+wDA1i655JLce++9Yx+DFVJVueCCC3L//fePfRQAAACAlWbxDhMkujOUBx54IBdeeOHYxwAAAABYacI7TJDozpAeeOCBsY8AAAAAsNKEdwAAAAAA6Eh4BwAAAACAjoR3mKCLL7547COwwi644IKxjwAAAACw0oR3mKB77rlHfGcQF1xwQe6///6xjwEAAACw0g6MfQBga/fcc09uuOGG/NAP/VDuv/9+K2UAAAAAmAmLdwAAAAAA6Eh4hwlrrSVJqmrkkwAAAAAA2yW8wwwI7wAAAAAwH8I7TNjpxTsAAAAAMB/CO8yAxTsAAAAAzIfwDhNm8Q4AAAAA8yO8w4R5c1UAAAAAmB/hHQAAAAAAOhLeYcIs3gEAAABgfoR3AAAAAADoSHiHCfPmqgAAAAAwP8I7AAAAAAB0JLzDhLXWXN8dAAAAAGZGeAcAAAAAgI6Ed5gwi3cAAAAAmB/hHQAAAAAAOhLeYcIs3gEAAABgfoR3AAAAAADoSHiHCbN4BwAAAID5Ed4BAAAAAKAj4R0mzOIdAAAAAOZHeAcAAAAAgI6Ed5gwi3cAAAAAmB/hHQAAAAAAOhLeYcIs3gEAAABgfoR3mDjhHQAAAADmRXiHCWutjX0EAAAAAGCHhHeYOIt3AAAAAJgX4R0mzOIdAAAAAOZHeIcJ8+aqAAAAADA/wjsAAAAAAHQkvMOEWbwDAAAAwPwI7wAAAAAA0JHwDhNm8Q4AAAAA8yO8AwAAAABAR8I7TJjFOwAAAADMj/AOAAAAAAAdCe8wYRbvAAAAADA/wjsAAAAAAHQkvMOEWbwDAAAAwPwI7wAAAAAA0JHwDhNm8Q4AAAAA8yO8AwAAAABAR8I7TJjFOwAAAADMj/AOAAAAAAAdCe8wYRbvAAAAADA/wjsAAAAAAHQkvMOEWbwDAAAAwPwI7zBxwjsAAAAAzIvwDhPWWhv7CAAAAADADgnvMHEW7wAAAAAwL8I7TJjFOwAAAADMj/AOE+bNVQEAAABgfoR3AAAAAADoSHiHCbN4BwAAAID5Ed4BAAAAAKAj4R0mzOIdAAAAAOZHeAcAAAAAgI4ODPnkVfWsJC9Nsj/JK1prP3rW/bV5/7OT3Jfkutba7w55prm76KKLcuLEibGPwR47vXpvrY18EgAAAADg4Qy2eK+q/UleluTqJFckeW5VXXHWw65O8pTNj+uT/OxQ51kFojsuOwMAAAAA0zfkpWaekeT21todrbX7k7w+yTVnPeaaJK9pG347ycGq+qIBzzRrojsAAAAAwPQNGd4vSfKhhc/v3rxtp49JVV1fVUer6uixY8e6HxQAAAAAAHoZMrxvdU2Msy9QvZ3HpLV2Y2vtcGvt8KFDh7ocDgAAAAAAhjBkeL87yRMWPr80yb3n8Rg2HTx4cOwjAAAAAADwMIYM7+9O8pSqelJVXZjkOUluPusxNyd5fm346iQfb6396YBnmrXjx4+L72uutc/6CyEAAAAAwMQcGOqJW2sPVtWLk7w9yf4kr2yt3VpVL9q8/+VJ3prk2UluT3JfkhcMdZ5Vcfz48bGPAAAAAADAEoOF9yRprb01G3F98baXL/y8JfnuIc8AAAAAAAB7achLzQAAAAAAwNoR3gEAAAAAoCPhHQAAAAAAOhLeAQAAAACgI+EdAAAAAAA6Et4BAAAAAKAj4R0AAAAAADoS3gEAAAAAoCPhHQAAAAAAOhLeAQAAAACgI+EdAAAAAAA6Et4BAAAAAKAj4R0AAAAAADoS3gEAAAAAoCPhHQAAAAAAOhLeAQAAAACgI+EdAAAAAAA6Et4BAAAAAKAj4R0AAAAAADoS3gEAAAAAoCPhHQAAAAAAOhLeAQAAAACgI+EdAAAAAAA6Et4BAAAAAKAj4R0AAAAAADoS3gEAAAAAoCPhHQAAAAAAOhLeAQAAAACgI+EdAAAAAAA6Et4BAAAAAKAj4R0AAAAAADoS3gEAAAAAoCPhHQAAAAAAOhLeAQAAAACgI+EdAAAAAAA6Et4BAAAAAKAj4R0AAAAAADoS3gEAAAAAoCPhHQAAAAAAOhLeAQAAAACgI+EdAAAAAAA6Et4BAAAAAKAj4R0AAAAAADoS3gEAAAAAoKNqrY19hh2pqmNJ7hz7HBPwuCQfHfsQsITXKFPnNcqUeX0ydV6jTJ3XKFPnNcrUeY0yZVN6fT6xtXZoqztmF97ZUFVHW2uHxz4HnIvXKFPnNcqUeX0ydV6jTJ3XKFPnNcrUeY0yZXN5fbrUDAAAAAAAdCS8AwAAAABAR8L7fN049gHgYXiNMnVeo0yZ1ydT5zXK1HmNMnVeo0yd1yhTNovXp2u8AwAAAABARxbvAAAAAADQkfA+Q1X1rKr6QFXdXlU/MPZ5WA9V9YSqekdVvb+qbq2q79m8/Ueq6p6qeu/mx7MXfs0Pbr5OP1BV37hw+1dV1e9v3vdvq6rG+J5YPVX1J5uvrfdW1dHN2z6/qn6tqv5o88eLFh7vNcqeqaqnLvxe+d6q+kRVfa/fRxlLVb2yqj5SVX+wcFu33zOr6hFV9Uubt/8/VXX5nn6DzN45XqM/XlX/X1W9r6reWFUHN2+/vKo+s/B76csXfo3XKIM4x2u02z/XvUbZrXO8Rn9p4fX5J1X13s3b/T7Knqpzd6aV+fdR4X1mqmp/kpcluTrJFUmeW1VXjHsq1sSDSb6vtfY3knx1ku9eeO39ZGvt6Zsfb02Szfuek+RpSZ6V5Gc2X79J8rNJrk/ylM2PZ+3h98Hqe+bma/Hw5uc/kOSW1tpTktyy+bnXKHuutfaB079XJvmqJPcleePm3X4fZQyvyme/dnr+nvnCJMdba389yU8m+bHBvhNW1avy2a/RX0vypa21L0/yh0l+cOG+Dy78Xvqihdu9RhnKq7L1P4N7/XPda5TdelXOeo221r5t4d9JfzXJGxbu9vsoe+lcnWll/n1UeJ+fZyS5vbV2R2vt/iSvT3LNyGdiDbTW/rS19rubP/9kkvcnuWTJL7kmyetba3/ZWvvjJLcneUZVfVGSx7bW3tU23mTiNUn+/rCnZ81dk+TVmz9/dc683rxGGdOV2fgPmzuXPMZrlEG11t6Z5P9v7/5D7izrOI6/P7imzml/ZFk6g6WOqJD5g1DWxjBbhiVYKRujbRXYRP8o/yiywAqCCIyKIEE2xZpDE8thlK6iomhps2EsrbVcNhyzGqRz7o+tb3+c+1lnT+c8+3U/5+w5z/sFY+dcz3Vf93XDl+u+n+9zXde9Z1xxm2Nmd1sPAe8em30kHY1eMVpVj1fVgebrJmDORG0Yo5pMfcbRfhxHNXATxWgTSzcC6ydqwxjVZJkgzzQyz6Mm3qee84C/d33fycTJT6l1zdKcS4DfNkW3Nst913YtAeoXq+c1n8eXS20o4PEkm5Pc1JSdU1W7oHNjB97QlBujGqalHP5LjuOoThZtjpmHjmkSpf8GXjdpPdd09DHgR13f5yb5fZJfJFnYlBmjGoa27uvGqCbTQmB3VW3rKnMc1VCMyzONzPOoifepp9dfZWrgvdC0lWQ2neVon6yql+gs57kAmA/sAu4cq9rj8JqgXGrDgqq6lM52XLckWTRBXWNUQ5FkJnAd8L2myHFUU8HxxKOxqkmT5HN0lqiva4p2AW+uqkuA24D7k5yFMarBa/O+boxqMi3j8IkgjqMaih55pr5Ve5Sd1OOoifepZydwftf3OcALQ+qLppkkr6EzGK6rqocBqmp3VR2sqv8Ad9PZDgn6x+pODl8SbAyrNVX1QvP/i3T2zn4nsLtZeja2TPLFproxqmF5H/BUVe0Gx1GddNocMw8dk2QG8FqOfksGqa8kK4H3A8ubJeU0y87/1XzeDGwH5mGMasBavq8bo5oUTTx9EHhgrMxxVMPQK8/ECD2Pmnifep4ELkoyt5kxtxTYMOQ+aRpo9sBaAzxTVV/rKn9TV7XrgbG3pW8AljZvkJ5L5+UWTzTLhF5OckXT5grgkYFchEZakjOSnDn2GVhCJx43ACubaiv5X7wZoxqWw2YXOY7qJNPmmNnd1oeBn40lSaXjleQa4DPAdVW1r6v89WMvWEvyFjox+ldjVIPW8n3dGNVkuRp4tqoObc/hOKpB65dnYoSeR2cM6kRqR1UdSHIr8BhwCrC2qrYOuVuaHhYAHwH+kGRLU3Y7sCzJfDpLdXYAnwCoqq1JHgT+SGcZ8C1VdbA57mY6b1c/nc6+nN17c0rH6xzg+817UmYA91fVj5M8CTyY5OPA88ANYIxqOJLMAt5DM1Y2vuo4qmFIsh5YDJydZCdwB/AV2hsz1wDfSfIXOjOLlg7gsjRC+sToZ4FTgY3NPX9TVa0GFgFfSnIAOAisrqqxGW3GqCZFnxhd3OJ93RjVCekVo1W1hv9/3xA4jmrw+uWZRuZ5NP4hSpIkSZIkSZKk9rjVjCRJkiRJkiRJLTLxLkmSJEmSJElSi0y8S5IkSZIkSZLUIhPvkiRJkiRJkiS1yMS7JEmSJEmSJEktMvEuSZIkDUGSOUkeSbItyfYk30gy8yiP/XmSyyehT4uTPDqu7L1JtjT/9ib5U/P5viSrk6xo6q1Kcu5k91GSJEmaCky8S5IkSQOWJMDDwA+q6iJgHjAb+HKPujNaON8px3tsVT1WVfOraj7wO2B5831FVd1VVfc1VVcB5/ZrR5IkSZpOTvghXpIkSdIxuwrYX1X3AFTVwSSfAp5LcgdwI3AtcBpwRpJrgXuAtwHPAKePNZRkCfBF4FRgO/DRqtqbZAewFlgCfCvJnj71rgG+DvwTeOpYLiLJF4C9wA7gcmBdkleBK8fV69nHYzmXJEmSNJU4412SJEkavLcDm7sLquol4HngwqboSmBlVV0F3Azsq6qL6cyKvwwgydnA54Grq+pSOjPSb+tqdn9VvQv4Sa96SU4D7gY+ACwE3ng8F1NVD3H4bPhXx352FH2UJEmSRo4z3iVJkqTBC1BHKN9YVXuaz4uAbwJU1dNJnm7Kr6AzC/7Xnd1rmAn8pqu9B45Q763Ac1W1DSDJd4GbTvTixjlSHyVJkqSRY+JdkiRJGrytwIe6C5KcBZxPZyuWy4BXxh3TL1G/saqW9TnPKxPVSzK/T7ttOlIfJUmSpJHjVjOSJEnS4P0UmJVkBRx6+emdwL1Vta9H/V8Cy5u67wAubso3AQuSXNj8bFaSeT2O71fvWWBukguaeieSHH8ZOPMYzi1JkiSNLBPvkiRJ0oBVVQHXAzck2Qb8GdgP3N7nkG8Ds5stZj4NPNG08w9gFbC++dkmOtvHjD9fz3pVtZ/O1jI/TPIr4G8ncFn3Ancl2ZLk0Mtfj7aPkiRJ0ihJ55lfkiRJkiRJkiS1wRnvkiRJkiRJkiS1yMS7JEmSJEmSJEktMvEuSZIkSZIkSVKLTLxLkiRJkiRJktQiE++SJEmSJEmSJLXIxLskSZIkSZIkSS0y8S5JkiRJkiRJUotMvEuSJEmSJEmS1KL/AiPMMt13D5pfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1872x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(26, 20))\n",
    "plt.title(\"Plotting 1-D array\")\n",
    "plt.xlabel(\"Ordered Title\")\n",
    "plt.ylabel(LCfraction)\n",
    "x = np.array(range(0, longueur))\n",
    "y = np.array(trie)\n",
    "plt.plot(x, y, color = \"black\", marker = \"o\", label = \"Array elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26666666664]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncf=[None] * (num_labels-1)\n",
    "for i in range(num_labels-1):\n",
    "    ncf[i] = trie[longueur * (i+1) // num_labels]\n",
    "ncf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    tokenized_sample = tokenizer(\n",
    "            sample[inputColumn],   #  postText + \". Paru dans \" + sample[\"Page name\"], #+ \", le \" + sample[\"Publish time\"]\n",
    "            #   str(int(sample[LCfraction])),\n",
    "            #padding=True,\n",
    "            #truncation=True,\n",
    "            #max_length=30\n",
    "            )\n",
    "    fraction = sample[LCfraction]\n",
    "    \n",
    "    if num_labels == 1:\n",
    "        tokenized_sample[\"label\"] = fraction\n",
    "    else:\n",
    "        tokenized_sample[\"label\"] = next((x for x, val in enumerate(ncf) if fraction < val), num_labels-1)\n",
    "    \n",
    "    \n",
    "    # print( fraction, tokenized_sample[\"input_ids\"], tokenized_sample[\"label\"], sample[inputColumn])\n",
    "    \n",
    "    return tokenized_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at Data/Curated\\cache-8bb57be11822c736.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_curated = curated.map(tokenize) #, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truthMean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.953800e+04</td>\n",
       "      <td>19538.000000</td>\n",
       "      <td>19538.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.314393e+17</td>\n",
       "      <td>0.324530</td>\n",
       "      <td>0.547241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.569423e+16</td>\n",
       "      <td>0.252824</td>\n",
       "      <td>0.497776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.041138e+17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.178346e+17</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.316563e+17</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.449404e+17</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.584642e+17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     truthMean         label\n",
       "count  1.953800e+04  19538.000000  19538.000000\n",
       "mean   8.314393e+17      0.324530      0.547241\n",
       "std    1.569423e+16      0.252824      0.497776\n",
       "min    8.041138e+17      0.000000      0.000000\n",
       "25%    8.178346e+17      0.133333      0.000000\n",
       "50%    8.316563e+17      0.266667      1.000000\n",
       "75%    8.449404e+17      0.466667      1.000000\n",
       "max    8.584642e+17      1.000000      1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_curated.to_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'postText': ['UK’s response to modern slavery leaving victims destitute while abusers go free', 'this is good', 'The \"forgotten\" Trump roast: Relive his brutal 2004 thrashing at the New York Friars Club'], 'id': [858462320779026432, 858421020331560960, 858368123753435136], 'targetTitle': ['‘Inexcusable’ failures in UK’s response to modern slavery leaving victims destitute while abusers go free, report warns', 'Donald Trump Appoints Pro-Life Advocate as Assistant Secretary of HHS for Public Affairs', 'The ‘forgotten’ Trump roast: Relive his brutal 2004 thrashing at the New York Friars Club'], 'targetDescription': ['“Inexcusable” failures in the UK’s system for dealing with modern slavery are\\xa0leaving victims reduced to destitution while their abusers go free because they are not adequately supported to testify against them, an alarming report has warned.', 'President Donald Trump has appointed pro-life advocate and former president of Americans United for Life Dr. Charmaine Yoest to be Assistant Secretary of Public Affairs for the Department of Health and Human Services.', \"President Trump won't be at this year's White House correspondents' dinner, but he once endured a tough roast in one of the most New York of rituals.\"], 'truthMean': [1.0, 0.13333333332, 0.399999999979999], 'input_ids': [[0, 17274, 26, 7, 57553, 47, 5744, 94859, 1294, 137802, 91519, 7, 8, 13480, 67, 12960, 128431, 4295, 738, 4092, 2], [0, 903, 83, 4127, 2], [0, 581, 44, 2472, 86828, 33, 58, 5879, 2062, 4438, 12, 853, 24056, 1919, 50029, 4821, 6, 118919, 54700, 99, 70, 2356, 5753, 9173, 21816, 7687, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'label': [1, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_curated[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_curated2 = tokenized_curated.train_test_split(splitFactor) # 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postText</th>\n",
       "      <th>id</th>\n",
       "      <th>targetTitle</th>\n",
       "      <th>targetDescription</th>\n",
       "      <th>truthMean</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Yes, I pelted stones yesterday. But that's no...</td>\n",
       "      <td>857124599988965376</td>\n",
       "      <td>21-Year-Old KashmirI Girl Who Pelted Stones At...</td>\n",
       "      <td>21-Year-Old Kashmir Girl Who Pelted Stones At ...</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>[0, 44, 91480, 4, 87, 4203, 3674, 3474, 1444, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A gallery of Miss Golden Globe: Looking back a...</td>\n",
       "      <td>818267827064938496</td>\n",
       "      <td>A Gallery of Miss Golden Globe: Looking Back a...</td>\n",
       "      <td>No doubt about it, Miss Golden Globe is the mo...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[0, 62, 6, 151570, 111, 16771, 43114, 162097, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What a world without Carrie Fisher means for t...</td>\n",
       "      <td>813846749597016064</td>\n",
       "      <td>What a world without Carrie Fisher means for t...</td>\n",
       "      <td>We will see Leia on screen one more time.</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>[0, 4865, 10, 8999, 15490, 3980, 5056, 169247,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Korea Sewol ferry disaster: Human remain...</td>\n",
       "      <td>846659966475882496</td>\n",
       "      <td>South Korea Sewol ferry disaster: Human remain...</td>\n",
       "      <td>Officials say bones have been found, as the re...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[0, 25134, 26320, 503, 25400, 1592, 1294, 6392...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Sean has revealed his fourth album \"I Deci...</td>\n",
       "      <td>812243109111361536</td>\n",
       "      <td>Big Sean Reveals Fourth Album 'I Decided,' Unl...</td>\n",
       "      <td>Sean tweeted the release date, artwork and sha...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 14195, 82421, 1556, 122273, 297, 1919, 227...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>\"So you want to preserve the liberal internati...</td>\n",
       "      <td>811323572635529216</td>\n",
       "      <td>So you want to preserve the liberal internatio...</td>\n",
       "      <td>As states retreat from preserving the U.S.-cre...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[0, 44, 8912, 398, 3444, 47, 479, 86687, 70, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>Awww.</td>\n",
       "      <td>822910938621902848</td>\n",
       "      <td>China Tightens Censorship, Bans Livestreaming ...</td>\n",
       "      <td>Ahead of the inauguration of Donald Trump as t...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[0, 62, 1574, 5, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>Donald Trump told a mostly-white PA crowd to t...</td>\n",
       "      <td>809631087681011712</td>\n",
       "      <td>Donald Trump Thanks African-Americans Who 'Did...</td>\n",
       "      <td>Trump said that African-Americans were &amp;quot;s...</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>[0, 16692, 5879, 30745, 10, 153161, 9, 148477,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>#Cabinet documents detail resistance to allowi...</td>\n",
       "      <td>815207640728936448</td>\n",
       "      <td>Cabinet documents detail resistance to allowin...</td>\n",
       "      <td>The Keating Government was warned that allowin...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>[0, 468, 441, 14508, 1179, 60525, 22443, 39746...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>Fatboy Slim: 'I've witnessed religious moments...</td>\n",
       "      <td>807601281904807936</td>\n",
       "      <td>Fatboy Slim: 'I've witnessed religious moments...</td>\n",
       "      <td>The superstar DJ on the magic of John Paul You...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[0, 24967, 30885, 82344, 12, 242, 568, 25, 272...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1953 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               postText                  id  \\\n",
       "0     \"Yes, I pelted stones yesterday. But that's no...  857124599988965376   \n",
       "1     A gallery of Miss Golden Globe: Looking back a...  818267827064938496   \n",
       "2     What a world without Carrie Fisher means for t...  813846749597016064   \n",
       "3     South Korea Sewol ferry disaster: Human remain...  846659966475882496   \n",
       "4     Big Sean has revealed his fourth album \"I Deci...  812243109111361536   \n",
       "...                                                 ...                 ...   \n",
       "1948  \"So you want to preserve the liberal internati...  811323572635529216   \n",
       "1949                                              Awww.  822910938621902848   \n",
       "1950  Donald Trump told a mostly-white PA crowd to t...  809631087681011712   \n",
       "1951  #Cabinet documents detail resistance to allowi...  815207640728936448   \n",
       "1952  Fatboy Slim: 'I've witnessed religious moments...  807601281904807936   \n",
       "\n",
       "                                            targetTitle  \\\n",
       "0     21-Year-Old KashmirI Girl Who Pelted Stones At...   \n",
       "1     A Gallery of Miss Golden Globe: Looking Back a...   \n",
       "2     What a world without Carrie Fisher means for t...   \n",
       "3     South Korea Sewol ferry disaster: Human remain...   \n",
       "4     Big Sean Reveals Fourth Album 'I Decided,' Unl...   \n",
       "...                                                 ...   \n",
       "1948  So you want to preserve the liberal internatio...   \n",
       "1949  China Tightens Censorship, Bans Livestreaming ...   \n",
       "1950  Donald Trump Thanks African-Americans Who 'Did...   \n",
       "1951  Cabinet documents detail resistance to allowin...   \n",
       "1952  Fatboy Slim: 'I've witnessed religious moments...   \n",
       "\n",
       "                                      targetDescription  truthMean  \\\n",
       "0     21-Year-Old Kashmir Girl Who Pelted Stones At ...   0.733333   \n",
       "1     No doubt about it, Miss Golden Globe is the mo...   0.800000   \n",
       "2            We will see Leia on screen one more time.    0.400000   \n",
       "3     Officials say bones have been found, as the re...   0.800000   \n",
       "4     Sean tweeted the release date, artwork and sha...   0.000000   \n",
       "...                                                 ...        ...   \n",
       "1948  As states retreat from preserving the U.S.-cre...   0.333333   \n",
       "1949  Ahead of the inauguration of Donald Trump as t...   0.600000   \n",
       "1950  Trump said that African-Americans were &quot;s...   0.133333   \n",
       "1951  The Keating Government was warned that allowin...   0.066667   \n",
       "1952  The superstar DJ on the magic of John Paul You...   0.333333   \n",
       "\n",
       "                                              input_ids  \\\n",
       "0     [0, 44, 91480, 4, 87, 4203, 3674, 3474, 1444, ...   \n",
       "1     [0, 62, 6, 151570, 111, 16771, 43114, 162097, ...   \n",
       "2     [0, 4865, 10, 8999, 15490, 3980, 5056, 169247,...   \n",
       "3     [0, 25134, 26320, 503, 25400, 1592, 1294, 6392...   \n",
       "4     [0, 14195, 82421, 1556, 122273, 297, 1919, 227...   \n",
       "...                                                 ...   \n",
       "1948  [0, 44, 8912, 398, 3444, 47, 479, 86687, 70, 2...   \n",
       "1949                                [0, 62, 1574, 5, 2]   \n",
       "1950  [0, 16692, 5879, 30745, 10, 153161, 9, 148477,...   \n",
       "1951  [0, 468, 441, 14508, 1179, 60525, 22443, 39746...   \n",
       "1952  [0, 24967, 30885, 82344, 12, 242, 568, 25, 272...   \n",
       "\n",
       "                                         attention_mask  label  \n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1  \n",
       "3         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1  \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0  \n",
       "...                                                 ...    ...  \n",
       "1948  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "1949                                    [1, 1, 1, 1, 1]      1  \n",
       "1950  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0  \n",
       "1951  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0  \n",
       "1952  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "\n",
       "[1953 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_curated2['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_curated3 = tokenized_curated2.remove_columns(removeColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_curated4 = tokenized_curated3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 1953\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 17585\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_curated4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_curated4.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    if num_labels == 1:\n",
    "        metric = load_metric(\"mse\")\n",
    "        return metric.compute(predictions=logits, references=labels)\n",
    "    elif num_labels == 2:\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "#    recall = recall_score(y_true=labels, y_pred=predictions)\n",
    "#    precision = precision_score(y_true=labels, y_pred=predictions)\n",
    "        f1 = f1_score(y_true=labels, y_pred=predictions)\n",
    "    # return metric.compute(predictions=predictions, references=labels) # , average = 'weighted'\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1 } #\" \"matthews_correlation\":matthews_correlation precision\": precision, \"recall\": recall, \"f1\": f1, \n",
    "    else:\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        metric = load_metric(\"matthews_correlation\") # matthews_correlation, accuracy \"glue\", \"mrpc\") # , \"sst2\") #  stsb de la ouatte de phoque\n",
    "        matthews_correlation = metric.compute(predictions=predictions, references=labels)\n",
    "        # accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "        # f1 = f1_score(y_true=labels, y_pred=predictions)\n",
    "        return matthews_correlation\n",
    "        # return {\"accuracy\": accuracy}.update(matthews_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(modelPath,\n",
    "                                  evaluation_strategy= \"steps\", # \"epoch\",\n",
    "                                  eval_steps = 50, # Evaluation and Save happens every 50 steps\n",
    "                                  save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "                                  num_train_epochs = 8,\n",
    "                                  optim= 'adamw_torch',\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  # learning_rate = 5e-4,\n",
    "                                  weight_decay=weight_decay,\n",
    "                                  push_to_hub=push_to_hub,\n",
    "                                  metric_for_best_model = 'accuracy' if num_labels == 2 else 'mse' if num_labels == 1 else 'matthews_correlation',\n",
    "                                  load_best_model_at_end=True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 44, 91480, 4, 87, 4203, 3674, 3474, 1444, 131101, 5, 4966, 450, 25, 7, 959, 2367, 87, 3444, 47, 54, 1242, 468, 4332, 16711, 18, 6, 118280, 2], [0, 62, 6, 151570, 111, 16771, 43114, 162097, 12, 157268, 4420, 99, 70, 170277, 4, 70, 160608, 13, 4, 136, 70, 67199, 7, 468, 122558, 33, 54285, 90, 2], [0, 4865, 10, 8999, 15490, 3980, 5056, 169247, 26950, 100, 70, 4612, 34210, 14997, 13, 2], [0, 25134, 26320, 503, 25400, 1592, 1294, 6392, 1515, 12, 28076, 47143, 7, 14037, 2], [0, 14195, 82421, 1556, 122273, 297, 1919, 22759, 927, 7156, 44, 568, 49132, 48141, 4, 58, 136, 51, 14507, 67175, 3525, 44, 9083, 3132, 58, 2], [0, 192429, 167540, 26156, 5281, 10, 242, 10770, 214, 47, 21629, 25, 40, 37631, 4, 136, 30309, 72856, 2363, 121584, 7086, 7, 5, 2], [0, 51661, 329, 35262, 4223, 65922, 7, 16692, 5879, 25, 7, 57553, 47, 6, 130687, 90791, 7, 98, 49002, 2], [0, 10160, 32, 2], [0, 581, 17955, 2795, 35691, 7, 4420, 13416, 111, 41550, 5458, 6431, 44, 2271, 20549, 2320, 58, 2], [0, 24372, 496, 138931, 23409, 15490, 468, 428, 13857, 20271, 10, 468, 4692, 29495, 214, 116816, 4, 70, 14799, 112, 68872, 959, 47, 468, 1727, 1294, 4049, 5, 2], [0, 40469, 189924, 527, 515, 5, 7674, 11387, 372, 13, 47, 89406, 136, 3229, 1221, 442, 7279, 32, 2], [0, 22392, 13038, 21072, 196212, 82421, 33365, 3443, 12, 44, 670, 474, 91177, 83, 8306, 47, 28127, 47, 398, 1242, 2], [0, 4865, 25, 7, 99, 145054, 23, 20703, 5, 5879, 25, 7, 25944, 47, 39531, 184, 984, 4028, 9, 156427, 209332, 98, 4620, 7, 2], [0, 468, 30513, 11387, 372, 13, 47, 348, 1294, 468, 168678, 138195, 7, 12, 242, 724, 126, 645, 442, 4, 108203, 13034, 9248, 4, 935, 108203, 1556, 21501, 3674, 25, 2], [0, 1061, 4, 3129, 83, 935, 185839, 49726, 32, 468, 201142, 25561, 468, 201142, 683, 28021, 468, 173466, 468, 90710, 420, 2347, 5636, 2], [0, 11246, 26775, 1221, 63769, 72095, 47, 46312, 50339, 390, 185, 25820, 41159, 111, 127501, 9, 218142, 157796, 7, 2], [0, 214526, 23040, 213566, 7, 1295, 70, 49119, 3229, 10, 9622, 647, 52875, 7, 604, 12960, 2412, 25, 7, 6867, 2], [0, 5879, 25, 7, 1439, 4241, 26783, 140526, 765, 2809, 6, 34590, 89829, 390, 43953, 13, 157075, 185256, 7, 2], [0, 2356, 99, 840, 94266, 1771, 6, 88322, 13450, 6431, 24, 4806, 41550, 134053, 390, 9090, 4341, 39544, 1314, 2], [0, 438, 37873, 18822, 71, 764, 4163, 11675, 645, 390, 10, 8, 56, 98, 7071, 9572, 929, 7, 25, 10013, 292, 24189, 5036, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'label': [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_curated4[\"train\"][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(model,\n",
    "                  training_args,\n",
    "                  train_dataset=tokenized_curated4[\"train\"],\n",
    "                  eval_dataset=tokenized_curated4[\"test\"],\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  # callbacks = [EarlyStoppingCallback(early_stopping_patience=3)] #3\n",
    "                  # optimizers=(torch.optim.AdamW, torch.optim.lr_scheduler.LambdaLR) \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1953\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1960\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='401' max='1960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 401/1960 05:24 < 21:08, 1.23 it/s, Epoch 1.63/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689014</td>\n",
       "      <td>0.546716</td>\n",
       "      <td>0.706938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689556</td>\n",
       "      <td>0.546716</td>\n",
       "      <td>0.706938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689811</td>\n",
       "      <td>0.546716</td>\n",
       "      <td>0.706938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688872</td>\n",
       "      <td>0.546716</td>\n",
       "      <td>0.706938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690372</td>\n",
       "      <td>0.546716</td>\n",
       "      <td>0.706938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688853</td>\n",
       "      <td>0.546716</td>\n",
       "      <td>0.706938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688878</td>\n",
       "      <td>0.546716</td>\n",
       "      <td>0.706938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1358' max='2199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1358/2199 00:25 < 00:16, 52.37 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 17585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17585\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# faire la boucle avec le trainer pytorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# %pdb\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\trainer.py:1475\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\trainer.py:1602\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1600\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 1602\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\trainer.py:2257\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2254\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   2256\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2257\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2258\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2261\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2267\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   2268\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m   2269\u001b[0m     speed_metrics(\n\u001b[0;32m   2270\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m     )\n\u001b[0;32m   2275\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\trainer.py:2431\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2428\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   2430\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 2431\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   2434\u001b[0m     xm\u001b[38;5;241m.\u001b[39mmark_step()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\trainer.py:2641\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   2639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels:\n\u001b[0;32m   2640\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[1;32m-> 2641\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2642\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m   2644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\trainer.py:2016\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2015\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2016\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1545\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1545\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1557\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1559\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    995\u001b[0m )\n\u001b[1;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    578\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    510\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    511\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 513\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\modeling_utils.py:2472\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   2469\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m-> 2472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:525\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 525\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 426\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PytorchGPU\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# faire la boucle avec le trainer pytorch\n",
    "# %pdb\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.43070757389068604,\n",
       " 'eval_accuracy': 0.8125,\n",
       " 'eval_f1': 0.8235294117647058,\n",
       " 'eval_runtime': 0.0941,\n",
       " 'eval_samples_per_second': 170.12,\n",
       " 'eval_steps_per_second': 21.265,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Sauver le modèle sur le disque\n",
    "model.save_pretrained(modelPath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer.save_vocabulary(modelPath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour voir si on fait de l'overfitting, rien de mieux que de dessiner le graphe avec le jeu d'entrainement\n",
    "overfitting = \"test\" #  \"train\" # \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model.to(torch.device(\"cuda:0\"))\n",
    "predictions = trainer.predict(tokenized_curated4[overfitting])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0825515 , -1.261397  ],\n",
       "       [ 0.16840985, -0.17182237],\n",
       "       [ 1.0844158 , -1.2639939 ],\n",
       "       [-1.0910052 ,  1.2826365 ],\n",
       "       [-1.0903685 ,  1.2806336 ],\n",
       "       [ 1.0841495 , -1.2615179 ],\n",
       "       [-1.0897197 ,  1.2802064 ],\n",
       "       [-1.0307432 ,  1.2115184 ],\n",
       "       [-1.0845279 ,  1.2736262 ],\n",
       "       [ 1.0609885 , -1.2291647 ],\n",
       "       [ 1.0843325 , -1.262958  ],\n",
       "       [ 1.0833747 , -1.2612356 ],\n",
       "       [ 1.0854067 , -1.2649003 ],\n",
       "       [-1.089214  ,  1.2787708 ],\n",
       "       [ 1.077905  , -1.254181  ],\n",
       "       [-1.0848985 ,  1.2747905 ]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.43070757389068604,\n",
       " 'test_accuracy': 0.8125,\n",
       " 'test_f1': 0.8235294117647058,\n",
       " 'test_runtime': 0.0844,\n",
       " 'test_samples_per_second': 189.484,\n",
       " 'test_steps_per_second': 23.685}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tct = tokenized_curated2[overfitting].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_labels == 1:\n",
    "    tct[\"labelC\"] = predictions.predictions\n",
    "else:\n",
    "    tct[\"labelC\"] = np.argmax(predictions.predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tct = tct.sort_values(LCfraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\"r\", \"g\", \"b\", \"c\", \"m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_labels == 1:\n",
    "    color = [val for val in tct[\"labelC\"]]\n",
    "else:\n",
    "    color = [palette[val] for val in tct[\"labelC\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1634f3da400>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1872x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(26, 20))\n",
    "plt.title(\"Plotting 1-D array\")\n",
    "plt.xlabel(\"Ordered Title\")\n",
    "plt.ylabel(LCfraction) #LCfraction\n",
    "x = np.array(range(0, tct.shape[0]))\n",
    "y = tct[LCfraction]\n",
    "plt.scatter(x, y, c = color, marker = \"o\", label = \"Array elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postText</th>\n",
       "      <th>id</th>\n",
       "      <th>targetTitle</th>\n",
       "      <th>targetDescription</th>\n",
       "      <th>truthMean</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>label</th>\n",
       "      <th>labelC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One portion of the Trump trade is alive and well</td>\n",
       "      <td>858403549478301696</td>\n",
       "      <td>Investors are loving the champion of the Trump...</td>\n",
       "      <td>The smallest companies in the US market have g...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 6561, 126826, 111, 70, 5879, 52350, 83, 92...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>An Indian woman explores an NRA convention</td>\n",
       "      <td>858317450017992704</td>\n",
       "      <td>From Gandhi to guns: An Indian woman explores ...</td>\n",
       "      <td>Guns are not a part of the culture of my homel...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 893, 42878, 46667, 88898, 7, 142, 541, 122...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facts that will be truly upsetting to '90s girls</td>\n",
       "      <td>858464162594172928</td>\n",
       "      <td>Facts That Will Be Truly Upsetting To '90s Girls</td>\n",
       "      <td></td>\n",
       "      <td>0.066667</td>\n",
       "      <td>[0, 132640, 7, 450, 1221, 186, 87607, 1257, 89...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sunderland relegation ‘my worst day in footbal...</td>\n",
       "      <td>858387587987365888</td>\n",
       "      <td>Sunderland relegation ‘my worst day in footbal...</td>\n",
       "      <td>David Moyes experienced the ‘worst day’ of his...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>[0, 79326, 53303, 47130, 125682, 204, 1176, 13...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Will Anthony Joshua thrill 90,000 of his count...</td>\n",
       "      <td>858382080941727744</td>\n",
       "      <td>Predictions: Who wins Anthony Joshua vs. Wladi...</td>\n",
       "      <td>Heavyweight world titleholder Anthony Joshua f...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>[0, 20255, 94825, 184311, 6, 68332, 1181, 2510...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hacker releases new episodes of 'Orange Is the...</td>\n",
       "      <td>858439565199249408</td>\n",
       "      <td>Hacker Releases New Episodes of 'Orange Is the...</td>\n",
       "      <td>The hacker who claims to have stolen the upcom...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>[0, 52922, 56, 54452, 7, 3525, 50094, 7, 111, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Giant hurricane\" on Saturn: See the first ima...</td>\n",
       "      <td>858301853855080448</td>\n",
       "      <td>\"Giant hurricane\" on Saturn: First images from...</td>\n",
       "      <td>NASA&amp;#039;s Cassini spacecraft snapped a view ...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>[0, 44, 724, 26865, 3587, 12056, 86, 58, 98, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Almost 20% of Pres. Trump's first 100 days wer...</td>\n",
       "      <td>858450499921731584</td>\n",
       "      <td>President Trump's first 100 days 'by the numbe...</td>\n",
       "      <td>calvin has this</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>[0, 78289, 5510, 12719, 111, 20703, 5, 5879, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If Republicans can’t get this lawmaker, Obamac...</td>\n",
       "      <td>858302188573085696</td>\n",
       "      <td>If Republicans Can’t Get This Lawmaker, Obamac...</td>\n",
       "      <td>Representative Fred Upton helped guide dozens ...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[0, 4263, 131161, 7, 831, 26, 18, 2046, 903, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>times are tough. should i turn off the heat a ...</td>\n",
       "      <td>858307544208605184</td>\n",
       "      <td>ESPN Layoffs at Leading Edge of the Coming ‘Sp...</td>\n",
       "      <td>ESPN Layoffs at Leading Edge of the Coming 'Sp...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>[0, 20028, 621, 143033, 5, 5608, 17, 15504, 57...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Seth Rogen and The Lonely Island hint at movie...</td>\n",
       "      <td>858388697582096384</td>\n",
       "      <td>Seth Rogen, Lonely Island Hint at Movie About ...</td>\n",
       "      <td>Following the disastrous news surrounding the ...</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>[0, 503, 927, 82261, 33, 136, 581, 2091, 86, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Trump really needs an economic boom. So far, h...</td>\n",
       "      <td>858329066159714304</td>\n",
       "      <td>Trump really needs an economic boom. So far, h...</td>\n",
       "      <td>\"Are you going to have 80-year-olds working at...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[0, 5879, 6183, 27117, 142, 25313, 77227, 5, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russian authorities detain dozens at anti-Puti...</td>\n",
       "      <td>858360136276086784</td>\n",
       "      <td>Russian authorities detain dozens at anti-Puti...</td>\n",
       "      <td>Protesters demand that the Russian president s...</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>[0, 102374, 207048, 8, 25500, 31236, 1755, 99,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Quentin Tarantino remembers Reservoir Dogs: 'I...</td>\n",
       "      <td>858372115564777472</td>\n",
       "      <td>Quentin Tarantino remembers Reservoir Dogs: 'I...</td>\n",
       "      <td>Director tells Tribeca Festival first public s...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[0, 5813, 19, 2311, 45501, 78244, 37629, 7, 17...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Inside North Korea's secret prisons</td>\n",
       "      <td>858460992073863168</td>\n",
       "      <td>Inside Kim Jong-un's camps of death: Former No...</td>\n",
       "      <td>A female guard (stock photo) at a North Korean...</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>[0, 187040, 23924, 26320, 25, 7, 23410, 101085...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UK’s response to modern slavery leaving victim...</td>\n",
       "      <td>858462320779026432</td>\n",
       "      <td>‘Inexcusable’ failures in UK’s response to mod...</td>\n",
       "      <td>“Inexcusable” failures in the UK’s system for ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0, 17274, 26, 7, 57553, 47, 5744, 94859, 1294...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             postText                  id  \\\n",
       "11   One portion of the Trump trade is alive and well  858403549478301696   \n",
       "14         An Indian woman explores an NRA convention  858317450017992704   \n",
       "0    Facts that will be truly upsetting to '90s girls  858464162594172928   \n",
       "5   Sunderland relegation ‘my worst day in footbal...  858387587987365888   \n",
       "12  Will Anthony Joshua thrill 90,000 of his count...  858382080941727744   \n",
       "2   Hacker releases new episodes of 'Orange Is the...  858439565199249408   \n",
       "7   \"Giant hurricane\" on Saturn: See the first ima...  858301853855080448   \n",
       "4   Almost 20% of Pres. Trump's first 100 days wer...  858450499921731584   \n",
       "1   If Republicans can’t get this lawmaker, Obamac...  858302188573085696   \n",
       "8   times are tough. should i turn off the heat a ...  858307544208605184   \n",
       "9   Seth Rogen and The Lonely Island hint at movie...  858388697582096384   \n",
       "10  Trump really needs an economic boom. So far, h...  858329066159714304   \n",
       "3   Russian authorities detain dozens at anti-Puti...  858360136276086784   \n",
       "13  Quentin Tarantino remembers Reservoir Dogs: 'I...  858372115564777472   \n",
       "15                Inside North Korea's secret prisons  858460992073863168   \n",
       "6   UK’s response to modern slavery leaving victim...  858462320779026432   \n",
       "\n",
       "                                          targetTitle  \\\n",
       "11  Investors are loving the champion of the Trump...   \n",
       "14  From Gandhi to guns: An Indian woman explores ...   \n",
       "0    Facts That Will Be Truly Upsetting To '90s Girls   \n",
       "5   Sunderland relegation ‘my worst day in footbal...   \n",
       "12  Predictions: Who wins Anthony Joshua vs. Wladi...   \n",
       "2   Hacker Releases New Episodes of 'Orange Is the...   \n",
       "7   \"Giant hurricane\" on Saturn: First images from...   \n",
       "4   President Trump's first 100 days 'by the numbe...   \n",
       "1   If Republicans Can’t Get This Lawmaker, Obamac...   \n",
       "8   ESPN Layoffs at Leading Edge of the Coming ‘Sp...   \n",
       "9   Seth Rogen, Lonely Island Hint at Movie About ...   \n",
       "10  Trump really needs an economic boom. So far, h...   \n",
       "3   Russian authorities detain dozens at anti-Puti...   \n",
       "13  Quentin Tarantino remembers Reservoir Dogs: 'I...   \n",
       "15  Inside Kim Jong-un's camps of death: Former No...   \n",
       "6   ‘Inexcusable’ failures in UK’s response to mod...   \n",
       "\n",
       "                                    targetDescription  truthMean  \\\n",
       "11  The smallest companies in the US market have g...   0.000000   \n",
       "14  Guns are not a part of the culture of my homel...   0.000000   \n",
       "0                                                       0.066667   \n",
       "5   David Moyes experienced the ‘worst day’ of his...   0.066667   \n",
       "12  Heavyweight world titleholder Anthony Joshua f...   0.066667   \n",
       "2   The hacker who claims to have stolen the upcom...   0.200000   \n",
       "7   NASA&#039;s Cassini spacecraft snapped a view ...   0.266667   \n",
       "4                                     calvin has this   0.266667   \n",
       "1   Representative Fred Upton helped guide dozens ...   0.333333   \n",
       "8   ESPN Layoffs at Leading Edge of the Coming 'Sp...   0.400000   \n",
       "9   Following the disastrous news surrounding the ...   0.466667   \n",
       "10  \"Are you going to have 80-year-olds working at...   0.600000   \n",
       "3   Protesters demand that the Russian president s...   0.733333   \n",
       "13  Director tells Tribeca Festival first public s...   0.800000   \n",
       "15  A female guard (stock photo) at a North Korean...   0.866667   \n",
       "6   “Inexcusable” failures in the UK’s system for ...   1.000000   \n",
       "\n",
       "                                            input_ids  \\\n",
       "11  [0, 6561, 126826, 111, 70, 5879, 52350, 83, 92...   \n",
       "14  [0, 893, 42878, 46667, 88898, 7, 142, 541, 122...   \n",
       "0   [0, 132640, 7, 450, 1221, 186, 87607, 1257, 89...   \n",
       "5   [0, 79326, 53303, 47130, 125682, 204, 1176, 13...   \n",
       "12  [0, 20255, 94825, 184311, 6, 68332, 1181, 2510...   \n",
       "2   [0, 52922, 56, 54452, 7, 3525, 50094, 7, 111, ...   \n",
       "7   [0, 44, 724, 26865, 3587, 12056, 86, 58, 98, 1...   \n",
       "4   [0, 78289, 5510, 12719, 111, 20703, 5, 5879, 2...   \n",
       "1   [0, 4263, 131161, 7, 831, 26, 18, 2046, 903, 2...   \n",
       "8   [0, 20028, 621, 143033, 5, 5608, 17, 15504, 57...   \n",
       "9   [0, 503, 927, 82261, 33, 136, 581, 2091, 86, 5...   \n",
       "10  [0, 5879, 6183, 27117, 142, 25313, 77227, 5, 1...   \n",
       "3   [0, 102374, 207048, 8, 25500, 31236, 1755, 99,...   \n",
       "13  [0, 5813, 19, 2311, 45501, 78244, 37629, 7, 17...   \n",
       "15  [0, 187040, 23924, 26320, 25, 7, 23410, 101085...   \n",
       "6   [0, 17274, 26, 7, 57553, 47, 5744, 94859, 1294...   \n",
       "\n",
       "                                       attention_mask  label  labelC  \n",
       "11            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      0       0  \n",
       "14                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      0       0  \n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      0       0  \n",
       "5   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0       0  \n",
       "12  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0       0  \n",
       "2   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0       0  \n",
       "7   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1       1  \n",
       "4   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1       1  \n",
       "1   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1       0  \n",
       "8   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1       1  \n",
       "9   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1       0  \n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1       0  \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1       1  \n",
       "13  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1       1  \n",
       "15                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]      1       1  \n",
       "6   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1       1  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    16.000000\n",
       "mean      0.437500\n",
       "std       0.512348\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       1.000000\n",
       "max       1.000000\n",
       "Name: labelC, dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tct[\"labelC\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truthMean</th>\n",
       "      <th>label</th>\n",
       "      <th>labelC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159879</td>\n",
       "      <td>-0.213001</td>\n",
       "      <td>0.073121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truthMean</th>\n",
       "      <td>0.159879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.770341</td>\n",
       "      <td>0.652858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>-0.213001</td>\n",
       "      <td>0.770341</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labelC</th>\n",
       "      <td>0.073121</td>\n",
       "      <td>0.652858</td>\n",
       "      <td>0.683130</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  truthMean     label    labelC\n",
       "id         1.000000   0.159879 -0.213001  0.073121\n",
       "truthMean  0.159879   1.000000  0.770341  0.652858\n",
       "label     -0.213001   0.770341  1.000000  0.683130\n",
       "labelC     0.073121   0.652858  0.683130  1.000000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tct.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "518d7075d1893c878292e2224ccc1059d061275fbe8ab229c7a260871a84e0e1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
